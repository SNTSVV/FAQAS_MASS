% !TEX root = MAIN.tex

\chapter{Data-driven Mutation Testing}
\label{chapter:datamutation}

% !TEX root = MutationTestingSurvey.tex

\section{Overview of the Data-driven Mutation Testing Process}
\label{sec:dataProcess}

	\begin{figure}
	\centering
		\includegraphics[width=\textwidth]{images/dataProcess}
		\caption{Data-driven Mutation Testing Process.}
		\label{fig:data:process}
	\end{figure}



This Chapter defines a test suite assessment process based on the injection of faults in the data processed by software components; we refer to this process as \INDEX{data-driven mutation testing}. 
%The definition of data-driven mutation testing is a unique contribution of this book; it has not been presented in previous literature work.

Data-driven mutation testing aims to assess test suites by simulating faults that affect the data produced, received, or exchanged by the software and its components.
It is based on a \INDEX{fault model} capturing the type of data faults that might affect the system. The fault model is produced by software engineers based on their domain knowledge and experience~\cite{di2015generating}. The considered faults might be due to programming errors, hardware problems, or critical situations in the environment (e.g., noise in the channel). The data is then automatically  mutated (i.e., modified) by a set of operators that aim to replicate the faults in the fault model. For example, the \INDEX{bit flip operator} flips a randomly selected bit of a field of the transmitted data (see Section~\ref{sec:faultModel}). 
%Mutation operators can be applied multiple times, on different data chunks or over repeated executions of a test case, ti


Figure \ref{fig:data:process} shows the reference \INDEX{data-driven mutation testing process} that will be considered in FAQAS. The process is based on two main sub-processes, \INDEX{test suite evaluation} and \INDEX{test suite augmentation}, which are described in Sections~\ref{sec:data:test_suite_evaluation}~and~\ref{sec:data:test_suite_augmentation}, respectively. Differently from the code-driven mutation testing process introduced in Section~\ref{sec:process}, the data-driven mutation testing process has not been formalized by existing software testing literature. An extensive discussion of related work has been presented in deliverable D1.

\MREVISION{C-P-29}{The type of faults that might be simulated by data-driven mutation testing are \INDEX{CPU faults}, \INDEX{memory faults}, \INDEX{data processing faults}, and \INDEX{communication faults}. 
However, it is worth noting that these faults are the means to perform mutation testing (i.e., evaluate test suites), they are not the purpose of data-driven mutation testing. Data-driven mutation testing does not aim at simulating such faults to determine if the system is robust but it aims to determine if, in the presence of such faults, the test suite fails as it would be expected. The choice of the faults to simulate depends on the type of test suite to evaluate.}

\CHANGED{In FAQAS we focus on evaluating the quality of \INDEX{functional test suites}. In the following, we briefly discuss the reasons why we do not evaluate \INDEX{robustness testing} test suites, which, among the different types of test suites, is the closest to mutation testing. Indeed,  it is often performed through mutation.
Space systems are expected to be robust with respect to CPU faults and memory faults; for this reasons such faults are often the means for performing robustness testing. Robustness testing is often performed by relying on ad-hoc fault injection systems (e.g., by corrupting memory through debugger features), which may require the test cases to be manual performed. For these reasons evaluating the quality of robustness test cases with an automated and generic strategy appear infeasible.}

%\CHANGED{To perform data-driven mutation testing of functional test suites all the different types of faults might be considered (e.g., CPU faults, memory faults, data processing faults, and communication faults). However, CPU faults and memory faults simply result into erroneous results, which are simulated by replacing valid values or performing bit flips.}

\CHANGED{With data-driven mutation testing we aim to simulate higher-level problems that typically affect complex data structures. The main reason is that other types of problems (e.g., computation of erroneous results) are already targeted by code-driven mutation. For this reason, we aim to perform data-driven mutation testing at the boundary of software components.
An ideal target for data-driven mutation testing are loosely coupled software components; typically the ones that run on separated pieces of hardware (e.g., the on board controller and the ADCS).
Despite other cases can be envisaged (e.g., pure software components that run on the same hardware), we target components running on separated hardware since we believe they are more likely affected by problems due to an incorrect understanding of requirements specifications or error-handling (e.g., because developed by separate teams).}
%Since data-driven mutation testing alters the data produced, received, or exchanged by the software or its components, 

\CHANGED{For this reason, in FAQAS, we apply data-driven mutation testing to evaluate test suites that trigger the execution and communication between multiple components (e.g., system or integration test cases). In FAQAS, data-driven mutation testing is not meant to be applied to assess unit test suites.}

\clearpage

\renewcommand\APPR{\emph{DAMAt}\xspace}

\section{Test Suite Evaluation} % (fold)
\label{sec:data:test_suite_evaluation}

\STARTCHANGEDWPT

\input{damat/introduction}
\input{damat/related}
\input{damat/approach}

\clearpage

\subsection{Data-driven mutation not based on buffers}



\MREVISION{C-P-31}{The communication between loosely coupled software components is performed by relying on APIs of a dedicated communication layer; which is typical in well designed software systems.
The functioning of such communication layer may vary from system to system. The \INDEX{communication layer} works by serializing and deserializing the data that should be transmitted on the communication channel. The goal of serialization/deserialization is to perform a data transformation, i.e, translate the representation of data from the format used inside the program (e.g., a data structure) into a low level format that can be transmitted on the channel (e.g., a stream of bytes or a memory buffer).}

\CHANGED{The differences that we may observe from system to system are related to the input interface of the communication layer. We may observe two cases:}

\CHANGED{\begin{itemize}
\item The communication layer provides serialization (deserialization) primitives that receive (produce) unstructured data (e.g, a memory buffer).
\item The communication layer provides serialization (deserialization) primitives that receive (produce) data structured according to a specific format. 
\end{itemize}}

\CHANGED{In both the two cases, the communication layer performs a \INDEX{data transformation}, i.e, translates the data from the format used inside the program (e.g., a data structure) into a low level format that can be transmitted on the channel (e.g., a memory buffer). However, this commonality does not enable the definition of a single solution to perform data mutation. More precisely, \EMPH{a solution that performs mutation on memory buffers may not be practical in both the two cases}. Indeed, to alter data that is already flattened on a low level representation it is necessary a (potentially complex) data model that describes how to load such data into a more structured representation that should drive the mutation. 
The data modelling effort would thus be redundant in case the data is structured according to a specific format.} 

\CHANGED{To minimize modelling costs, in the presence of a complex data structure, the data model should coincide with the data structure defined in the programming language used to implement the system (or the modelling language from which the program has been derived), while the fault model should be defined as an extension of such data structure (e.g., through annotations).
Modelling of data flattened into a low level representation is feasible only when this is already the input format of the communication layer (in such cases the transmitted data is not expected to follow a complex structure).
Finally, the definition of a generic data loading solution 
that loads data from a memory buffer into a more structured representation and works with any data structure, might be infeasible. 
%structures (e.g, fields of variable size and multiple dependencies among fields), 
%might be particularly expensive.
}

\CHANGED{In FAQAS industrial case studies, the \INDEX{communication layer} is implemented in-house by the company that produced the case study.
The system works by processing a flat structure stored in  a buffer array, which is the reason why the FAQAS \APPR solution focuses on buffers. However, space systems may rely on the ASN.1 compiler architecture; in this case, the data processed by the compiler is highly structured. The definition of the data structure is provided as an ASN.1 grammar that is then translated by the ASN.1 compiler into a C structure. For this reason, \EMPH{in FAQAS, we also design a preliminary solution for highly structured data defined with ASN.1}. Even if the ASN.1 data is translated into a C structure, we believe that a generic solution that rely on data models defined according to C structures might not be feasible. Indeed, data structures may contain elements with complex dependencies. For example, a tree data structure may define the tree depth  in a specific data field and programmers may assume that pointer to child nodes are not read when the max depth is not reached (i.e., pointer child pointers in leaf nodes are not NULL). Specifying such logic into a generic framework is particularly hard if not infeasible.}




\CHANGEDNOV{We believe that the technology implementing \INDEX{data mutation} should depend on the type of system under test. This is mostly due to the need for (1) implementing mutation operations that are fast and (2) reducing the amount of data-modelling to be manually performed (ideally engineers would like to reuse existing models and artefacts). Based on the case studies shared for WP2, we observe that data-driven mutation testing might be performed by modifying either data that is stored in an array or in a data structure defined through the ASN.1 grammar. In our vision, these two solutions differ for the strategy used to model the data and for the algorithms implemented to execute mutation. Despite a more general solution (e.g., based on UML models) that glue together these two strategies might be feasible, its implementation might be the target of an ESA activity. Indeed, when data modelling is based on generic high-level models it is necessary to implement a layer that translate high-level representation into low-level data that can be efficiently processed at runtime. The following subsections describe the two distinct cases; however, in FAQAS, we will focus on the implementation of a solution for buffer arrays. The main reasons are two (1) buffer arrays appear to be a common strategy for implementing data communication (2) in FAQAS we lack case studies based on the ASN.1 grammar (see Section~\ref{sec:caseStudies:ASN:data}). In the following, however, we provide the results of a preliminary study concerning the design of a data-driven method for the ASN.1 grammar.}

\ENDCHANGEDWPT

\clearpage

\subsubsection{Fault Model Specifications for ASN.1 grammar}
\label{subsub:asn1model}

The ASN.1 grammar enables engineers to specify data structures where the types of the items in the data structure are selected from a predefined set.

%When the data to be mutated is stored in a data structure defined through the ASN.1 grammar, the fault model is specified by indicating which operators to apply on the specific fields of the data structure. 

We have identified a set of feasible fault classes for each type supported by the ASN1SCC compiler.
The corresponding mutation operators are automatically configured based on the ASN.1 grammar (e.g., in the case of an attribute of type INTEGER, the min/max values of the VOR operator are derived from the boundaries of the INTEGER type).
Table~\ref{table:faultModel:FAQAS:ASN1} provides, for each of such types, the feasible fault classes and the configurations for the mutation operators.
In the configuration for the mutation operators, we refer to the variables (e.g., MIN and MAX) appearing in the ASN.1 xml file.

Figure~\ref{fig:ASN1ProbesGeneration} provides an overview of the process in place to generate probes including the fault model.
The engineer first export the ASN.1 grammar as XML, then he modifies the generated file by specifying, for each \emph{Asn1Type}, the mutation operator to be used (this is done by adding an xml attribute called \emph{MutationOperator} with a value specifying the name of the operator). 

An example is provided in Listings~\ref{asnXML} and \ref{asnXMLUpdated}. Listing~\ref{asnXML} provides the xml generated by the grammar, which includes two INTEGER types.
Listing~\ref{asnXMLUpdated} provides the xml updated by the engineer, who indicates that the two integers should be mutated with the VAR and the VOR operator. To tune the operators, the engineer updates the MIN and MAX values for those integers to capture only nominal values. 
In the case of the first integer (the one to be mutated with VAR), the engineer sets 5 as MAX.
In the case of the second integer (the one to be mutated with VOR), the engineer sets MIN and MAX to 0 and 50 respectively.
%The engineer can tune the mutation by changing the value ranges associated to the different types. For example, this could be done to restrict the valid range of an INTEGER from (MIN=-100, MAX=100) to a nominal range of (MIN=0,MAX=50).
In case a data type is defined through value range constraints, the FAQAS framework will configure one mutation operator instance for each range.

\input{tables/table_faultModelASN1}

Figure~\ref{fig:ASN1ProbesGeneration} shows that, finally, the FAQAS toolset generates a modified version of the ASN1 source code containing the serializer and deserializer functions. The generated source code contains the FAQAS API functions to mutate the data in ASN.1 data types. Examples are shown in the following sections (see Figure~\ref{ASN_mutations}).

\begin{figure}[h]
  \centering
    \includegraphics[width=12cm]{images/ASN1mutationProces}
      \caption{Data-driven probes generation process for ASN1.}
      \label{fig:ASN1ProbesGeneration}
\end{figure}


\input{listings/asnXML.tex}
\input{listings/asnXMLupdated.tex}




\clearpage
\subsection{FAQAS Data Mutation API and Probes}
\label{sec:FAQASDataMutationProbes}

In FAQAS, the data-driven mutation testing API is automatically generated from the fault model provided by engineers. \INDEX{Data mutation probes} are either manually implemented by software engineers (in the case data mutation should target an ad-hoc communication layer that works with data buffers) or automatically generated by the toolset (in the case data mutation should target an ASN.1-based communication layer).



\subsubsection{Data Mutation Probes for ASN.1}
\label{sec:FAQASDataMutationProbesASN}

%\DONE{This section still needs to be written. We may put a sequence diagram that show that at the beginning the probe loads the info about the mutation operation instance to execute and execute it if feasible.}

%Fabrizio: I removed the picture because it does not help the reader
%\begin{figure}[tb]
%  \centering
%    \includegraphics[width=\textwidth]{images/DataDrivenASNProcess}
%      \caption{Data-driven mutation process for ASN.1 grammars.}
%      \label{fig:DataDrivenASNProcess}
%\end{figure}


After the generation of the extended ASN.1 source code according to the 
the fault model definition process provided in Figure~\ref{fig:ASN1ProbesGeneration}, 
\MREVISION{C-P-37}{the FAQAS framework will automatically modify the extended ASN.1 deserializer (or serializer) code to insert calls to the FAQAS mutation API.
Particularly, the framework will insert one 
invocation of the automatically generated FAQAS mutation API for each data type to be mutated.}
Listings~\ref{ASN_encode} shows an example of a probe added at the beginning of function \emph{TypeNested\_encode} to mutated the TypeNested data to be encoded by function \emph{TypeNested\_encode}.
Listings~\ref{ASN_decode} shows an example of a probe added 
at the end of function \emph{TypeNested\_decode}
to mutated the TypeNested data decoded by function \emph{TypeNested\_decode}.

The insertion of the probe in the serializer code is useful when the fault model is not modified by the engineer but simply includes the boundary values automatically generated by the FAQAS framework. 
This is useful to generate invalid data to be serialized and thus test the capability of the ASN.1 serializer to detect illegal values.

The insertion of the probe in the deserializer code is useful to simulate the generation of invalid data from a faulty component. This is useful when the fault model had been modified by the engineer to reflect possible non-nominal cases with values belonging to the legal value domain.

%Figure~\ref{fig:DataDrivenASNProcess} provides an overview of the mutation process followed by the ASN.1 data mutation functions.
%At runtime, for each mutation operator instance, a single probe will be enabled. 

Similarly to the case of data mutation for data buffers, each mutant can implement a single mutation operation instance or work as a mutant schemata where the mutation operation instance is selected at runtime, based on a configuration parameter. Each mutation operator instance is identified by a unique identifier. 


%Fabrizio: You never introduced E, its not a good example!
%A MOI represents the mutation to be applied. More specifically, it contains the data type name to be mutated, and an ID that represents the mutation operator to be applied, and under what conditions applies.

%For example Listing~\ref{ASN_mutations} shows two possible MOI probes for the E data type. 
%\texttt{E\_1} exercises a mutation for the E data type when the value of \texttt{pVal} is less than or equal to 255. If the condition is true, then the value is modified by replacing it for the MAX value (e.g., 255).
%Similarly, \texttt{E\_2} exercises the E data type when the value of \texttt{pVal} is equal to 1299. If the condition applies, then the value is replaced by $1299 + 1$ (e.g., VAT mutation operator).
%The mutation is saved after its execution, so it is not performed twice. 

\input{listings/ASN_decode}


Listing~\ref{ASN_mutations} shows three possible mutation operation instances for the \emph{TypeNested} data type configured according to the fault model shown in Listing~\ref{asnXMLUpdated}.
\texttt{TypeNested\_1} applies the VAR operator,
 \texttt{TypeNested\_2} applies the VOR operator by setting the data value below the lower bound.
 \texttt{TypeNested\_3} applies the VOR operator by setting the data value above the upper bound.
\input{listings/ASN_mutations}


\clearpage



\clearpage
\section{Test Suite Augmentation} % (fold)
\label{sec:data:test_suite_augmentation}

\STARTCHANGEDWPT

The \INDEX{test suite augmentation process} concerns the definition of additional test cases to increase the mutation score.
It consists of four activities \INDEX{Identify Test Inputs}, \INDEX{Generate Test Oracles}, \INDEX{Execute the SUT}, \INDEX{Fix the SUT}. 
Despite these activities match the ones performed in the case of code-driven mutation testing, they are triggered and implemented in a different manner, as described below.

In the presence of mutants not killed by test cases (i.e., when the  \INDEX{mutation score} is not equal to 100\%), engineers are expected to manually investigate the underlying problems. Indeed, as reported in Section~\ref{sec:mutationAnalysisResults}, two might be the reasons for a low MS: poor oracle quality and missing test input sequences (i.e., the software does not reach the state in which it could kill the mutant).
For the first case (poor oracle quality), manual work is needed because automated approaches to automatically generate test oracles in the presence of system or integration test suites are not available. For the second case, existing test generation approaches (e.g., KLEE) might suffer from scalability problem that prevent bringing the system into a desired state ; also, they cannot deal with systems whose components communicate through channels. For this reason, generating test oracles and fixing the SUT (in case a fault is discovered after test suite augmentation) shall be performed manually.

When mutation operators are not applied because of the lack of appropriate data to mutate (i.e., in the presence of fault model coverage and mutation operation coverage below 100\%), engineers are expected to generate new test inputs for the SUT that enable the application of all the mutation operators. 
However, the methodology to adopt may vary based on the test objective and the system architecture. 
We discuss the case of the producer-consumer and client-server architecture, two common software architectures. We leave the discussion of other architectures (e.g., broker architecture and event-bus architecture) to future work.

In Figures~\ref{fig:dataDrivenTestSuiteAugmentationC} to~\ref{fig:dataDrivenTestSuiteAugmentationE}, we exemplify the two architectures. In both the two cases, data-driven mutation may concern the generated data and occur either on the component that generates the data (Figure~\ref{fig:dataDrivenTestSuiteAugmentationC}), or on the component that receives the data (Figure~\ref{fig:dataDrivenTestSuiteAugmentationD}).
For the client-server case, instead, data mutation may concern also the request for data and be performed either on the client or the server (Figure~\ref{fig:dataDrivenTestSuiteAugmentationE}). For the producer-consumer case, static program analysis may be employed to automatically generate the missing data; to this end, we aim to rely on an \INDEX{extended data mutation probe}. For the client-server case, the \INDEX{extended data mutation probe} may still be used but only to generate message requests; therefore, it would be useful only when data-driven analysis is performed on the  request message. We exemplify the two cases below.

\begin{figure}[h]
  \centering
    \includegraphics[width=14cm]{images/dataDrivenTestSuiteAugmentationC}
      \caption{Data-driven mutation analysis for different architectures.}
      \label{fig:dataDrivenTestSuiteAugmentationC}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=14cm]{images/dataDrivenTestSuiteAugmentationD}
      \caption{Data-driven mutation analysis for different architectures.}
      \label{fig:dataDrivenTestSuiteAugmentationD}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=14cm]{images/dataDrivenTestSuiteAugmentationE}
      \caption{Data-driven mutation analysis for different architectures.}
      \label{fig:dataDrivenTestSuiteAugmentationE}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=14cm]{images/dataDrivenTestSuiteAugmentationB}
      \caption{Data-driven mutation analysis for different architectures.}
      \label{fig:dataDrivenTestSuiteAugmentationB}
\end{figure}

\ENDCHANGEDWPT

\clearpage
\subsection{Producer-consumer}


We assume to have a system that exchanges data of type TypeNested defined by relying on the ASN.1 grammar (see Listings~\ref{asnXMLUpdated} and ~\ref{asnXMLUpdated}). Also, we assume that the objective of data-driven mutation testing is to assess the quality of the test cases implemented to verify the consumer component. 
Such test cases may consist of sending predefined data through a producer component and verify that the consumer generates the expected output. 
To perform data-driven mutation, we may rely on a probe installed on the deserializer component. 


To enforce the generation of the required data types, we can augment the producer component with an extended mutation probe (called \emph{\_FAQAS\_TypeNested\_cover} in Listing~\ref{ASN_encode}). In this case the probe should not be used to mutate the data but it should include assertions that enable reachability analysis. Listing~\ref{ASN_encodeReachable} shows an example where, for each mutation operation implemented in the probe, we introduce an \emph{assert(false)} statement. Static analysis tools (e.g., KLEE)  can then be used to find inputs that enable reaching any of these assertions from the entry point of the producer component. For each assertion, the static analysis component will look for an input of the entry point (e.g., the main function) that enables reaching the assertion, i.e., generate data that can be mutated according to the provided mutation operation. The identified inputs can then be used to augment the test suite.

\input{listings/ASN_decodeReachable.tex}

\clearpage
\subsection{Client-server}

\STARTCHANGEDWPT

For the client-server case, we rely on the libParam case study provided by GSL. Listing~\ref{GSLmutate} shows the mutation probe, which is inserted into function \emph{gs\_rparam\_process\_packet}, on the server side. The probe mutates the buffer \emph{v\_General}, which contains a copy of a message request (i.e., \emph{request}). In the case of GSL, the FVAT operator configured to mutate \emph{request-$\>$table\_id} cannot be applied (i.e., MOC is not equal to 100\%); this indicates that the test cases do not cover a scenario in which the client passes a \emph{table\_id} above the threshold. To generate such a test case we may rely on the extended probe combined with \INDEX{static program analysis}. 

Listing~\ref{GSLcover} shows how the INDEX{extended mutation probe} might be inserted into the code of libParam. In practice, it requires the engineer to know the portion of code that handles the generation of a request message. Unfortunately, injecting the mutation probe is not sufficient to enable test generation but engineers need also to prepare a test template to enable test generation with KLEE. Listing~\ref{GSLtest} shows an example of such template based on existing libParam test cases; such test case requires the initialization of a number of state variables, which limits the possibility to automate its definition. For this reason, within FAQAS we did not find it feasible to automate data-driven mutation analysis with a tool but we aim to evaluate its manual feasibility in WP4.

Finally, when data-driven mutation is applied to the data generated by the server, test automation is made unfeasible by the fact that KLEE cannot work in the presence of a communication channel within the code to be analyzed. Such shortcoming is not observed when we mutate request data because the extended mutation probe is installed only on the client; the producer-consumer case is not affected by such shortcoming because, in this case, the probe is installed on the producer. Alternative test generation tools or extensions of KLEE shall be considered to overcome such limitations.

\input{listings/GSL_mutateNotCovered.tex}

\input{listings/GSL_mutateCoverage.tex}

\input{listings/GSL_test.tex}

\ENDCHANGEDWPT

%For example, in the case of the example in Figure~\ref{fig:DataDrivenSimpleExample}, engineers would need to implement test cases that trigger the exchange of \emph{DataMessages}.
%Fully automated approaches to generate test cases for data-driven mutation testing are unavailable; however, techniques that generate input data from scratch~\cite{gligoric2010test} or augment input data~\cite{DiNardo:TOSEM:2017} can be adopted. 
%Also, when the data used by test cases is generated by simulators, meta-heuristic search can be used to drive the generation of input data~\cite{Abdessalem:ICSE:2018}. 
%
%The execution of the SUT and the repair of the SUT are performed manually as in the case of code-driven data mutation.
%
%
%\TODO{Clarify if we generate test cases or not}
%
%Section~\ref{sec:testGenerationData} provides details about the existing solutions to  \emph{Identify Test Inputs} and \emph{Generate Test Oracles}.


\input{toolsetsEvaluationDataDriven}


