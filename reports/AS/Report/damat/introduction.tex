% !TEX root =  ../MAIN.tex
\subsection{Introduction}

Space systems, similarly to other \INDEX{Cyber Physical Systems} (CPSs), are heterogeneous systems that integrate computation, networking, and physical processes that are deeply interlaced~\cite{Khaitan:2015}. 
%As a result, CPSs development requires the adoption of a wide range of modeling formalisms to deal with discrete events~\cite{Lee1999}, multimodal dynamics \cite{Jha2010}, continuous time \cite{Lee2005}, and process networks \cite{Lee1995}. 
%CPSs' components are implemented with a wide range of programming languages to address different needs such as hardware requirements, time-based computation, and event-based computation.
\CHANGED{In CPSs, conformance with requirements is verified 
through test cases 
executed at different development stages, based on available development artifacts~\cite{Roehm:2019}. In this paper, we focus on the identification of faults in the executable software to be deployed on the CPS, and thus target  software-in-the-loop (SIL) and hardware-in-the-loop (HIL) testing.}
 
 When software systems are large and integrate a diverse set of components, it is difficult to ensure that the test suite can detect any latent severe fault. To ensure \UPDATED{test suites' quality}, standards for safety-critical software provide methodological guidance, for example through structural coverage adequacy; however, those strategies do not directly measure the fault detection capability of a test suite. A more direct solution to evaluate \UPDATED{test suites' quality} is \INDEX{mutation analysis}~\cite{jia2010analysis,papadakis2019mutation}. It consists of automatically generating faulty software versions and %measuring the quality of a test suite in terms of 
 \UPDATED{computing} the mutation score, that is, the percentage of faulty software versions detected (i.e., leading to a test failure).
 Mutation analysis is a good candidate to assess \UPDATED{test suites' quality} because there is a strong association between high mutation scores and high fault revelation capability for test suites~\cite{papadakis2018mutation}. 
 %which is not true for structural coverage measures~\cite{Chekam:17}.

Most mutation analysis techniques rely on the generation of faulty software through mutation operators that modify the software implementation (either the source or the executable code). 
Unfortunately, such techniques suffer from two major limitations which are critical in the CPS context: (1) they cannot identify problems related to the interoperability of integrated components (integration testing) and (2) they can be applied only to components that can be executed in the development environment.
In CPSs, major problems may arise because of the lack of  \INDEX{interoperability of integrated components}~\cite{Givehchi:2017,Jirkovsk:2017}, mainly due to the wide variety and heterogeneity of the technologies and standards adopted.
Also, there is  limited work on integration testing automation~\cite{Abbaspour:2015}, thus forcing companies to largely rely on manual approaches, which are error prone and likely to lead to incomplete test suites. 
It is thus of fundamental importance to ensure the effectiveness of test suites with respect to detecting interoperability issues, for example by making sure test cases trigger the exchange of all possible data items and report failures when erroneous data is being exchanged by software components. For example, the test suite for the control software of a satellite shall identify failures due to components working with different measurement systems~\cite{MarsClimateOrbiter}.
Unfortunately, well known, code-driven mutation operators (e.g., the sufficient set~\cite{delamaro2014designing,delamaro2014experimental}) simulate algorithmic faults by introducing small changes into the source code and are thus unlikely to simulate interoperability problems resulting in \UPDATED{exchanges of erroneous data}. 


The second limitation of code-driven mutation analysis approaches concerns \emph{the incapability of injecting faults into black-box components} whose implementation is not tested within the development environment (e.g., because it is simulated or executed on the target hardware).
For example, in a satellite system, such components include the control software of the Attitude Determination And Control System (ADCS), the GPS, and the Payload Data Handling Unit (PDHU). During SIL testing, the results generated by such components (e.g., the GPS position) are produced by a simulator. As for HIL testing, these components are directly executed on the target hardware and cannot be mutated, either because they are off-the-shelf components 
%(common for PDHU or GPS) 
or 
to avoid damages potentially introduced by the mutation.
% (e.g., for ADCS developed in-house).
%Interface operators~\cite{delamaro2001interface} may simulate data errors; however, they suffer from the other limitation of code-driven mutation analysis, that is, code-driven mutation analysis \emph{can be applied only to components that can be executed in the development environment}.

%FABRIZIO: probably we shall move the following paragraph directly to the related work..
%FABRIZIO: I decided to leave it here
An alternative to code-driven mutation analysis approaches are model-based ones, which mutate models of the software under test (SUT). 
%The mutants obtained by model-based approaches are generally used to drive the generation of test cases~\cite{He2011,Aichernig2015,Devroey2016,BELLI201625}. 
%Since models are high-level abstractions of the system under test --- usually capturing the main requirements of the system under test, the generated test cases shall identify critical faults due to mistakes in the implementation of requirements. 
Unfortunately, existing approaches do not include strategies to simulate interoperability problems; also, their primary objective is to support test generation not the evaluation of test suites~\cite{He2011,Aichernig2015,Devroey2016,BELLI201625}. 
Furthermore, model-based test generation may not be cost-effective if detailed models of the system under test are not available---which is the case of system models produced in early development stages---and lack key information required for testing (e.g., which telecommands shall trigger a state transition in a satellite system). 
%
% For example, 
%%it is unlikely to achieve model-based test generation for satellite systems when test cases consists of sequences of ground-to-orbit PUS telecommands and simulator configurations. 
%based on our experience with industry partners in the space business, we observed that model-based test generation is not feasible when the simulators being used do not rely on modelling standards or 

To address the above-mentioned limitations, we propose \INDEX{data-driven mutation analysis}, 
%a new type of model-based mutation analysis 
a new mutation analysis paradigm
that alters the data exchanged by software components in a CPS to evaluate the capability of a test suite to detect interoperability faults. Also, we present a technique, \INDEX{data-driven mutation analysis with tables} (\APPR),
to automate data-driven mutation analysis by relying on
a fault model that captures, for a specific set of components, both the characteristics of the data to mutate (e.g., the size and structure of the messages generated by the ADCS) and the types of fault that may affect such data (e.g., a  value out of the nominal range). The latter is formalized as a set of parameterizable mutation operators. 
Based on discussions with practitioners, to simplify adoption, we rely on fault models in tabular form where each row specifies, for a given data item, what mutation operator (along with its corresponding parameter values) to apply to which elements of the data item.
%Also, inspired by related work on model-based fault injection~\cite{PeachMozilla,di2015generating}, we have defined a catalog of data faults that shall be selected by engineers when specifying fault models. 
%Also, we have implemented a set of mutation operators that alter faults for a different set of data types.
At runtime, \APPR modifies the data exchanged by components according to the provided fault model (e.g., replaces a nominal voltage value with a value out of the nominal range).

%Data-driven mutation analysis relates to early work on abstract mutation analysis~\cite{Offutt2006} and inherits from it three test suites adequacy citeria, which are,  
%Mutation Coverage,
%% (i.e., for each mutant $m$, it is necessary that the test suite kills m),
%Mutation Operator Coverage,
%% (i.e., for each mutation operator, it is necessary create a mutated string $m$ that is derived using the mutation operator),
%and Mutation Production Coverage.
%% (i.e., for each mutation operator, and each production that the operator can be applied to, it is necessary to create a mutated string from that production).


%We performed an empirical evaluation of \APPR to determine the \UPDATED{effectiveness, feasibility, and applicability of data-driven mutation analysis for evaluating test suites.}
%%(2) compare it with traditional code-driven mutation testing. 
%Our benchmark consists of software for CPSs in the space domain provided by our industry partners, which are 
%an intergovernmental space agency, a world-renowned manufacturer and supplier of nanosatellites, and 
%a European developer of infrastructure products (e.g., microsatellites) and solutions for space.
%%the European Space Agency~\cite{ESA}, 
%%\GomSpace{} (\ONE), a manufacturer and supplier of nanosatellites\CITONE, and
%%\LuxSpace{} (\TWO), a developer of infrastructure products (e.g., microsatellites) and solutions for space\CITTWO.
%More specifically, the benchmark includes (1) the on-board embedded 
%{software system} for \SAIL{}\CITSAIL{}, a maritime microsatellite recently launched into space, and 
%(2) a configuration library used in constellations of nanosatellites~\cite{satSurvey}. 
%\UPDATED{Our empirical results show that \APPR (1)  successfully identifies different types of shortcomings in test suites, (2) prevents the introduction of equivalent and redundant mutants, and (3) is practically applicable in the CPS context.}

%This paper proceeds as follows \TODO{...}



%and (3) a mathematical library for flight software~\cite{MLSF}.
%, and 
%(4) a space certifiable compiler~\cite{ASN1CC} for the ASN.1 language, which is a standard interface description language for defining data structures that can be serialized and deserialized across platforms. 




%to commands or to generate test cases (i.e., sequences of inputs that enable us to distinguish the original from the mutated model).

%Interoperability faults might be simulated by model-based mutation analysis, which consists of systematically altering test models. The mutant models generate invalid test inputs that are executed against the SUT. The goal of MBMT is to determine if an invalid test case pass (i.e., if an invalid test case produces the output specified by the original model) thus revealing unexpected behavior in the SUT (i.e., a fault). 
%
%Hence, MBMT can expose faults that are caused by missing requirements or incorrect implementation. 
%
%The first work on model-based mutation testing was done by Budd and Gobal~\cite{Budd1985} for predicate- calculus specifications.
%Over the years, it has been applied to several formalisms, including probabilistic finite state machines [16], action system specifications~\cite{Aichernig2011}, UML state machines~\cite{Aichernig2015}, and Simulink models~\cite{He2011,Brillout2010}. Below, we report approaches relying on MBMT.
%
%
%
%For example, our work does not aim to assess if test suites can detect faults concerning the communication between heterogeneous components or the interoperability of different technologies and tools, two typical CPS problems.
%
% —especially in the factory floor—entail signifi- cant complexity for this integration
% relevance for to CPS software.
%Indeed,
%
%the wide variety and heterogeneity of the technologies and standards on which different CPS components rely, —especially in the factory floor—entail signifi- cant complexity for this integration
%
%This paper focused on investigating and identifying practical solutions to address the scalability of mutation analysis and the pertinence of mutation scores as an adequacy criterion in the context of embedded software for CPS. Important work remains concerning specific subjects in embedded software. For example, our work does not aim to assess if test suites can detect faults concerning the communication between heterogeneous components or the interoperability of different technologies and tools, two typical CPS problems. To address such issues, our future work includes the definition of mutation operators that alter the data exchanged by software components instead of their implementation
%
%Evaluate if the test suite can detect complex, high-level faults that lead to errors in structured data
%complex, high-level faults are not handled by redundancy mechanisms and should lead to test failures when observed
%e.g., when replacing a value for the nominal case (e.g., temperature above zero) with a value for a non-nominal case (e.g., below zero) a test failure should be observed
%simple faults already addressed by code-driven mutation testing
%code-driven mutation testing cannot generate high-level faults 
%traditional mutation operators simulate typos/simple errors
%higher-order operators inapplicable when data is generated by a simulator 
%no solution triggering faults for specific data types (e.g., order of data in array, values out of nominal range)
%low-level hardware problems are targeted by robustness testing
%
%
%The mutation score, i.e., the percentage of killed mutants, is a quantitative measure of the quality of a test suite. Recent studies have shown that achieving a high mutation score improves significantly the fault detection capability of a test suite
%~\cite{papadakis2018mutation},  
%\JMR{1.6}{
%a result which contrasts with that of structural coverage measures~\cite{Chekam:17}. However, a very high mutation score (e.g., above 75\%) is required to achieve a higher fault detection rate than the one obtained with other coverage criteria, such as statement and branch coverage~\cite{Chekam:17}. In other words, .}
%
%
%Such a conformance testing approach should effectively check conformance on already available development artifacts, e.g., abstract models built early in the development process.
%
%With data-driven mutation testing we emulate data that should trigger a change in the state of the system. Consequently, such data should trigger a failure in test cases. We do not aim to generate invalid data (e.g., bit burst) that should trigger redundancy mechanisms that transparently handle the generated data, which is the objective of fault injection.
%
%With traditional, code-driven, mutation testing, it is not possible to simulate such cases because the generation of data that triggers a change in state is unlikely to be achieved with simple changes in the source code. Also, in the CPS context, where may components (e.g., ADCS board) are simulated, code-driven mutation testing is not feasible (e.g., the code that generates the data is not available for mutation).
%
%Execution scenarios
%
%while executing a test case that exercises a nominal scenario, data-driven mutation testing lead to data characterizing a non-nominal scenario; in such a case the SUT is expected to return data that is different than the expected one (e.g., previous values, or error flags). Consequently the test suite should fail.
%
%while executing a test case that exercises a non-nominal scenario, data-driven mutation testing generates data characterizing a nominal scenario; in such a case the SUT is expected to return data that is different than the expected one 