% !TEX root =  Main.tex
\chapter{State-of-the-art}
\label{ch:stateOfTheArt}

%\item The literature on mutation analysis/testing mostly focuses on modifying the code of the software under test (hereafter, code-driven approaches). Some approaches rely on modifying models, but they aim to generate test cases not assessing test suites. There are no approaches that assess test suites by changing the data generated by software components (hereafter, data-driven approaches).
%\item The mutation operators widely adopted to perform code-driven mutation analysis are the sufficient set of operators and the set of deletion operators. Other operators did not receive the same degree of attention in the literature. For example, higher-order mutation operators are reported to be easier to kill than the first-order ones (i.e., they are less effective in assessing test suites limitations); consequently, they had been adopted less in empirical studies.
%o	The literature lacks mutation analysis approaches that enable simulating errors in the presence of simulated components (e.g., sensors).
%o	Scalable approaches targeting mutation testing (i.e., automatically generating test cases that kill mutants) are few. The most promising ones rely on symbolic execution based on LLVM, which might be inapplicable for onboard flight software compiled for specific architectures.

This chapter briefly discusses the applicability, in the context of space software, of existing solutions related to the four contributions of FAQAS: code-driven mutation analysis,
code-driven mutation testing, data-driven mutation analysis,
data-driven mutation testing.

\section{Code-driven mutation analysis}
% \label{sec:background}

Mutation analysis can drive the generation of test cases, which is referred to as \INDEX{mutation testing} in the literature.
A detailed overview of mutation testing and analysis solutions and optimizations can be found in recent surveys~\cite{jia2010analysis,papadakis2019mutation}.

\subsection{Mutation Adequacy and Mutation Score computation}
\label{background:adequacy}
A mutant is said to be killed if at least one test case in the test suite fails when exercising the mutant.
Mutants that do not lead to the failure of any test case are said to be live.
Three conditions should hold for a test case to kill a mutant: \emph{reachability} (i.e, the test case should execute the mutated statement), \emph{necessity} (i.e., the test case should reach an incorrect intermediate state after executing the mutated statement), and \emph{sufficiency} (i.e., the final state of the mutated program should differ from that of the original program)~\cite{offutt1997automatically}.

The mutation score, i.e., the percentage of killed mutants, is a quantitative measure of the quality of a test suite. Recent studies have shown that achieving a high mutation score improves significantly the fault detection capability of a test suite
~\cite{papadakis2018mutation},
\JMR{1.6}{
a result which contrasts with that of structural coverage measures~\cite{Chekam:17}. However, a very high mutation score (e.g., above 75\%) is required to achieve a higher fault detection rate than the one obtained with other coverage criteria, such as statement and branch coverage~\cite{Chekam:17}. In other words, there exists a strong association between a high mutation score and a high fault revelation capability for test suites.}


The capability of a test case to kill a mutant also depends on the observability of the program state. To overcome the limitations due to observability, different strategies to identify killed mutants can be adopted; they are known as strong, weak, firm, and flexible mutation coverage~\cite{ammann2016introduction}.
\JMR{1.11 3.9}{With strong mutation, to kill a mutant, there shall be an observable difference between the outputs of the original and mutated programs.
With weak mutation, the state (i.e., the valuations of the program variables in scope) of the mutant shall differ from the state of the original program, after the execution of the mutated statement~\cite{Lee1994}.
With firm mutation, the state of the mutant shall differ from the state of the original program at execution points between the first execution of the mutated statement and the termination of the program~\cite{Woodward88}.
Flexible mutation coverage consists of checking if the mutated code leads to object corruption~\cite{mateo2012validating}}.
For space software, we suggest to rely on strong mutation because it is the only criterion that truly assesses the fault detection capability of the test suite; indeed, it relies on a mutation score that reflects the percentage of mutants leading to test failures. With the other mutation coverage criteria, a mutant is killed if the state of the mutant after  execution of the mutated statement differs from the one observed with the original code, without any guarantee that either the erroneous values in state variables will propagate or the test oracles will detect them.




\subsection{Mutation Operators}
\label{sec:related:operators}

%\input{tables/operators}


Mutation analysis introduces small syntactical changes into the code (source code or machine code) of a program through a set of mutation operators that simulate programming mistakes.



The  \emph{sufficient set of operators} is widely used for conducting empirical evaluations ~\cite{offutt1996experimental,rothermel1996experimental,andrews2005mutation,kintis2017detecting}.
The original sufficient set, defined by Offutt et al., is composed of the following operators: Absolute Value Insertion (ABS), Arithmetic Operator Replacement (AOR), Integer Constraint Replacement (ICR), Logical Connector Replacement (LCR), Relational Operator Replacement (ROR), and Unary Operator Insertion (UOI)~\cite{offutt1996experimental}.
Andrews et al.~\cite{andrews2005mutation} have also included the
\emph{statement deletion operator} (SDL)~\cite{delamaro2014designing}, which ensures that every pointer-manipulation and field-assignment statement is tested.

The sufficient set of operators enables an accurate estimation of the mutation score of a test suite~\cite{siami2008sufficient}; furthermore, the mutation score computed with the sufficient set is a good estimate of the fault detection rate (i.e., the portion of real faults discovered) of a test suite~\cite{andrews2005mutation,Just:RealFaults:2014}.

However, empirical work has shown that, to maximize the detection of real faults, a set of operators should be used in addition to the sufficient set: Conditional Operator Replacement (COR),
Literal Value Replacement (LVR), and Arithmetic Operator Deletion (AOD)~\cite{Kintis2018}.


\CHANGED{The SDL operator has inspired the definition of mutation operators (e.g., \emph{OODL operators}) that delete portions of program statements, with the objective of replacing the sufficient set with a simpler set of mutation operators.
The OODL mutation operators include the delete Arithmetic (AOD), Bitwise (BOD), Logical (LOD), Relational (ROD), and Shift (SOD) operators.}
\JMR{3.8}{Empirical results show that deletion operators produce significantly fewer equivalent mutants\footnote{\JMRCHANGE{For example, statement deletion can lead to equivalent mutants only if statements are redundant, which is unlikely~\cite{Offut:2013}.}}}
\cite{delamaro2014designing,delamaro2014experimental} and, furthermore,
test suites that kill mutants generated with both SDL and OODL operators kill a very high percentage of all mutants (i.e., 97\%)~\cite{delamaro2014experimental}.

Another alternative to the sufficient set of operators is the generation of \emph{higher order mutants}, which result from the application of multiple mutation operators for each mutation~\cite{jia2009higher,kintis2010evaluating,offutt1992investigations,papadakis2010empirical}. However, higher order mutants are \CHANGED{easier to  kill
than the first order ones (i.e., less effective to assess test suites limitations)}~\cite{papadakis2010mutation,papadakis2019mutation}, and there is
\CHANGED{limited empirical evidence regarding which mutation operators should be combined to resemble real faults and minimize the number of redundant mutants~\cite{papadakis2019mutation}.}



\subsection{Compile-time Scalability}
\label{sec:compile:time}


\JMR{1.12}{The potentially large size of the software under test, combined with the large number of available mutation operators, may make the compilation of all  mutants infeasible.}

To reduce the number of invocations to the compiler to one, \emph{mutant schemata} include all the mutations into a single executable~\cite{untch1993mutation}.
With mutant schemata, the mutations to be tested are selected at run-time through configuration parameters. This may lead to a compilation speed-up of 300\% \cite{papadakis2010automatic}.


Another solution to address compile-time scalability issues consists of \emph{mutating machine code}  (e.g., binary code~\cite{becker2012xemu}, assembly language~\cite{crouzet2006sesame},
Java bytecode~\cite{ma2006mujava},
 and
.NET bytecode~\cite{derezinska2011object}), thus avoiding the execution of the compilation process after creating a mutant.
A common solution consists of mutating the
 LLVM Intermediate Representation (IR) \cite{hariri2016evaluating},
which enables the development of mutants that work with multiple programming languages~\cite{hariri2019comparing} and facilitates the integration of optimizations based on dynamic program analysis~\cite{denisov2018mull}.


Unfortunately, the mutation of machine code
may lead to mutants that are not representative of real faults \JMRCHANGE{(i.e., faults caused by human mistakes at development time)} because they are impossible to generate from the source code\JMR{3.10}{~\cite{denisov2018mull}.
For instance, a function invocation in the source code may lead to hundreds of machine code instructions (e.g., the function call \emph{std::vector::push\_back} leads to 200 LLVM IR instructions) and, consequently, some of the mutants derived from such instructions cannot be derived by mutating the source code.}
In the case of IR mutation, some of these impossible mutants can be automatically identified~\cite{denisov2018mull}; however,
the number of generated mutants tend to be higher at the IR level than at the source code level, which may reduce scalability~\cite{hariri2019comparing}.
 In addition, we have encountered three problems that prevented the application of
 mutation analysis tools based on LLVM IR to our case study systems.
First, space software relies on compiler pipelines (e.g., RTEMS~\cite{RTEMS}) that include architecture-specific optimizations not supported by LLVM.
Second, there is no guarantee that the executables generated by LLVM are equivalent to those produced by the original compiler.
 Third, efficient toolsets based on LLVM often perform mutations dynamically~\cite{denisov2018mull}, which is infeasible when the software under test needs to be executed within a dedicated simulator, a common situation with space software and many other types of embedded software in cyber-physical systems.




\subsection{Runtime Scalability}
\label{sec:scalability}

A straightforward mutation analysis process consists of executing the full test suite against every mutant; however, it may lead to scalability problems in the case of a large software under test (SUT) with expensive test executions.
\emph{Simple optimizations} that can be applied to space software consist of (S1) stopping the execution of the test suite when the mutant has been killed, (S2) executing only those test cases that cover the mutated statements~\cite{delamaro1996proteum}, and (S3) rely on timeouts to automatically detect infinite loops introduced by mutation~\cite{papadakis2019mutation}.

\emph{Split-stream execution} consists of generating a modified version of the SUT that creates multiple processes (one for each mutant) only when the mutated code is reached \cite{king1991fortran,tokumoto2016muvm}, thus saving time and resources. Unfortunately, it cannot be applied in the case of space software that needs to run with simulators because, in general, the hosting simulator cannot be forked by the hosted SUT.

Another feasible solution consists of  \emph{randomly selecting a subset of the generated mutants}~\cite{zhang2010operator,gopinath2015hard,zhang2013operator}.
Zhang et al. \cite{zhang2013operator} empirically demonstrated that a random selection of 5\% of the mutants is sufficient for
estimating, with high confidence, the mutation score obtained with the complete mutants set.
Further,
they show that sampling mutants uniformly across different program elements (e.g., functions)
leads to a more accurate mutation score prediction than sampling mutants globally in a random fashion.
For large software systems that lead to thousands of mutants, random mutation analysis is the only viable solution. However, for very large systems such as the ones commonly found in industry, randomly selecting 5\% of the mutants may still be too expensive.

\CHANGEDOCT{Gopinath et al. estimate the number of mutants required for an accurate mutation score~\cite{gopinath2015hard}.
They rely on the intuition that, under the assumption of independence between mutants,
mutation analysis can be seen as a Bernoulli experiment in which the outcome of the test for a single mutant is a Bernoulli trial (i.e., mutant successfully killed or not) and, consequently,
the mutation score should follow a binomial distribution.
They rely on Tchebysheff’s inequality~\cite{Tchebichef1867} to find a theoretical lower bound on the number of mutants required for an accurate mutation score.
More precisely, they suggest that, with 1,000 mutants,
the estimated mutation score differs from the real mutation score at most by 7 percentage points.
However, empirical results show that the binomial distribution provides a conservative estimate of the population variance and, consequently, 1,000 mutants enable in practice a more accurate estimate ($> 97\%$) of the mutation score than expected.}

In the statistics literature, the correlated binomial model~\cite{Bahadur},
and related models~\cite{Kupper1978,NG:ModifiedBinomialDistributions:1989,VanDerGeest:2005} are used when Bernoulli trials are not independent~\cite{Zhang:CrrelatedFirearm:NIST:2019}.
In our work, based on the results achieved by Gopinath et al., we assume that the degree of correlation between mutants is limited and the binomial distribution can be used to accurately estimate the mutation score.
%which is supported by our empirical results (see Section~\ref{sec:evaluation}).
%  In Appendix B,
%  %~\ref{appendix:correlation},
%  we verify the correctness of our assumptions
%%based on the analysis of the distribution of the mutation score in our empirical evaluation.
%%In addition, our assumptions are demonstrated as being true by our empirical results (see Section~\ref{sec:evaluation}).}
%by reporting the degree of association between trials and by comparing the  probability mass function for the binomial and the correlated binomial distributions, for all our subjects.}



The statistics literature also provides a number of approaches for the computation of a sample size (i.e., the number of mutants, in our context) that enables estimates with a given degree of accuracy~\cite{Krejcie,Cochran,Bartlett,Krishnamoorthy07}.
For binomial distributions, the most recent work is that of Gonçalves et al.~\cite{Goncalves2012}, that
determines the sample size by
relying on heuristics for the computation of confidence intervals for binomial proportions.
A confidence interval has a probability $p_c$ (the confidence level) of including the estimated parameter (e.g., the mutation score).
Results show that the largest number of samples required to compute a 95\% confidence interval is 1,568.

If used to drive the selection of mutants, both the approaches of Gopinath et al. and Gonçalves et al., which suggest sampling at least 1,000 mutants, may be impractical when mutants are tested with large system test suites.


\CHANGED{An alternative to computing the sample size before performing an experiment is provided by sequential analysis approaches, which determine the sample size while conducting a statistical test~\cite{waldSequential}.
Such approaches do not perform worst-case estimates and may thus lead to smaller sample sizes. For example, the sequential probability ratio test, which can be used to test hypotheses, has been used in mutation analysis as a condition to determine when to stop test case generation (i.e., when the mutation score is above a given threshold)~\cite{Hsu:90}. In our context, we are interested in point estimation, not hypothesis testing; in this case,
the sample size can be determined through a fixed-width sequential confidence interval (FSCI), i.e., by computing the confidence interval after every new sample and then stop sampling when the interval is within a desired bound~\cite{Frey:FixedWidthSequentialConfidenceIntervals:AmericanStatistician:2010,Chen2013,Yaacoub:OptimalStopping}.
Concerning the method used to compute the confidence interval in FSCI, the statistics literature~\cite{Frey:FixedWidthSequentialConfidenceIntervals:AmericanStatistician:2010} reports that the Wald method~\cite{WaldMethodVollset} minimizes the sample size but requires an accurate variance estimate. We will therefore resort to a non-parametric alternative, which is Clopper-Pearson~\cite{ClopperPearson}.
Note that FSCI
has never been applied to determine the number of mutants to consider in mutation analysis.}

Other solutions to address \emph{runtime scalability problems} in mutation analysis  aim to \emph{prioritize test cases} to maximize the likelihood of executing first those that kill the mutants~\cite{just2012using,papadakis2011automatically,zhang2013faster}. The main goal is to save time by preventing the execution of a large subset of the test suite, for each mutant.
Previous work aimed at prioritizing faster test cases~\cite{just2012using} but this
 may not be adequate with system-level test suites whose test cases have similar, long execution times.
Approaches that rely on data-flow analysis to identify and prioritize the test cases that likely satisfy the killing conditions~\cite{papadakis2011automatically} are prohibitively expensive and are unlikely to scale to large systems.
Other work~\cite{zhang2013faster} combines three coverage criteria:
(1) the number of times the mutated statement is exercised by the test case,
 (2) the proximity of the mutated statement to the end of the test case
 (closer ones have higher chances of satisfying the sufficiency condition)
 , and (3)
 the percentage of mutants belonging to the same class file of the mutated statement that were already killed by the test case.
Criterion (3) is also used to reduce the test suite size, by only selecting the test cases above a given percentage threshold.
Unfortunately, only criterion (1) seems applicable in our context; indeed, criterion (2) is ineffective with system test cases whose results are checked after long executions, while criterion (3) may be inaccurate when only a random, small subset of mutants is executed, as discussed above.






%For \INDEX{test case reduction}, the idea is to remove those test cases that are somehow redundant (e.g., test cases that when removed from the test suites do not change the mutation score).
%Usaola et al. \cite{usaola2012reduction} proposed a greedy algorithm that iteratively selects  the test cases that kill most of the mutants that were not killed by the previously selected test cases.
%%\DONE{No change to do here. However please keep them in mind for the current work.}
%Shi et al. \cite{shi2014balancing} assessed the effects of reducing the size of test suites with an experiment on 18 projects with a total of 261\,235 test cases. Their results show that \emph{it is possible to maintain constant the mutation score and reduce test suite size without loss in the \emph{fault detection rate}}.
%On the same line, Zhang et al. \cite{zhang2013faster} suggest to define a subset of tests of the original test suite and to run the mutants against the subset, their assumption is that if the mutants cannot be killed by the subset also the original test suite will not be able to kill the mutants.
%
%
%




\subsection{Detection of Equivalent Mutants}
\label{sec:background:equivalent}

\JMR{1.13}{A mutant is equivalent to the original program when they both generate the same outputs for the same inputs.}
Although identifying equivalent mutants is an undecidable problem~\cite{madeyski2013overcoming,Bugg:Correctness:82}, several heuristics have been developed to address it.

The simplest solution consists of relying on \emph{trivial compiler optimisations}~\cite{papadakis2015trivial, kintis2017detecting,papadakis2019mutation}, i.e., compile both the mutants and the original program with compiler optimisations enabled and then determine whether their executables match. In C programs, compiler optimisations can reduce the total number of mutants by 28\%~\cite{kintis2017detecting}.



Solutions that identify equivalent mutants based on \emph{static program analysis} (e.g., concolic execution~\cite{holling2016nequivack,Chekam2021} and bounded model checking~\cite{riener2011test})
show promising results \JMR{2.3}{(e.g., to automatically identify non-equivalent mutants for batch programs~\cite{Chekam2021})} but
they rely on static analysis solutions that cannot work with system-level test cases that execute with hardware and environment simulators in the loop.
Indeed, (1) simulation results cannot be predicted by pure static analysis,  (2) concolic execution tools, which rely on LLVM, cannot be run if the SUT executable should be generated with a specific compiler (see Section~\ref{sec:compile:time}),
\JMR{2.3}{ (3) there are no solutions supporting the concolic execution of large software systems within simulation environments
(state-of-the-art techniques work with small embedded software~\cite{Herdt2019}), and (4) communication among components not based on direct method invocations (e.g., through network or databases) is not supported by existing toolsets.}

Alternative solutions rely on \emph{dynamic analysis} and compare data collected when testing the original software and the mutants~\cite{grun2009impact,schuler2010covering,schuler2013covering,schuler2009efficient}.
The most extensive empirical study on the topic shows that nonequivalent mutants can be detected by counting the number of methods (excluding the mutated method) that, for at least one test case, either (1) have statements that are executed at a different frequency with the mutant, (2) generate at least one different return value, or (3) are invoked at a different frequency~\cite{schuler2013covering}. To determine if a mutant is non-equivalent, it is possible to define a threshold indicating the smallest number of methods with such characteristics. A threshold of one identifies non-equivalent mutants with an average precision above 70\% and an average recall above 60\%. This solution outperforms more sophisticated methods relying on dynamic invariants~\cite{schuler2009efficient}. Also, coverage frequency alone leads to results close to the ones achieved by including all three criteria above~\cite{schuler2013covering}.
However, such approaches require some tailoring
because collecting all required data
(i.e., coverage frequency for every program statement, return values of every method, frequency of invocation of every method) has a computational and memory cost that may break real-time constraints.

\subsection{Detection of Redundant Mutants}
\label{sec:background:redundant}
Redundant mutants are either \emph{duplicates}, i.e., mutants that are equivalent with each other but not equivalent to the original program, or \emph{subsumed}, i.e., mutants that are not equivalent with each other but are killed by the same test cases.

Duplicate mutants can be detected by relying on the same approaches adopted for equivalent mutants.

According to Shin et al., subsumed mutants should not be discarded but analyzed to augment the test suite with additional test cases that fail with one mutant only~\cite{Shin:TSE:DCriterion:2018}.
The augmented test suite has a higher
 fault detection rate than a test suite that simply satisfies mutation coverage; however, with large software systems the approach becomes infeasible because of the lack of scalable test input generation approaches.

\input{FR_toolsetsEvaluation.tex}

\subsection{Summary}
\label{sec:back:summary}

We aim to rely on the sufficient set of operators since it has been successfully used to generate a mutation score that accurately estimates the fault detection rate for software written in C and C++, languages commonly used in embedded software.
%Based on recent results, we should however extend the sufficient set with LVR, and all the operators belonging to OODL.
\CHANGED{Further, since recent results have reported on the usefulness of both LVR and OODL operators to support the generation of test suites with high fault revealing power~\cite{Kintis2018}, the sufficient set may be extended to include these two operators as well.}

To speed up mutation analysis by reducing the number of mutants, we should consider the SDL operator alone \CHANGED{or in combination with the OODL operators}. However, such heuristic should be carefully evaluated to determine the level of confidence we can expect.

Among compile time optimizations, only mutant schemata appear to be feasible with space software.
Concerning scalability, simple optimizations (i.e., S1, S2, and S3 in Section~\ref{sec:scalability}) are feasible. Alternative solutions are the ones relying on mutant sampling and coverage metrics.
However, to be applied in a safety or mission critical context, mutant sampling approaches should provide guarantees about the
level of confidence one may expect. Currently, this can only be achieved with approaches requiring a large number of sampled mutants (e.g., $1,000$).  Therefore, sequential analysis based on FSCI, which minimizes the number of samples and provides accuracy guarantees, appears to be the most appropriate solution in our context.
Further, test suite selection and prioritization strategies based on code coverage require some tailoring to cope with real time constraints.
% when mutant sampling rates are below 5\% should be evaluated.
%Furthermore, code coverage metrics that are feasible for space software need to be defined.

Equivalent mutants can be identified through trivial compiler optimizations and the analysis of coverage differences; however, it is necessary to define and evaluate appropriate coverage metrics. The same approach can be adopted to identify duplicate mutants. The generation of test cases that distinguish subsumed mutants is out of the scope of this work.

\JMR{1.1 3.3}{A high-level description of a possible mutation testing pipeline was proposed in a recent survey\footnote{The main objective of such pipeline was to walk the reader through the survey, not to propose a precise and feasible solution.}~\cite{papadakis2019mutation}. It consists of the following sequence of activities: select (sample) mutants, compile mutants, remove equivalent and redundant mutants, generate test inputs that kill mutants, execute mutants, compute mutation score, reduce test suites and prioritize test cases.
Unfortunately, such pipeline does not enable the integration of many optimizations proposed above, which further motivates our work. For example, it cannot support FSCI-based sampling, which requires mutants sampling to be coupled with mutants execution. Also, it does not envision the detection of equivalent and redundant mutants based on code coverage. Moreover, it only partially addresses scalability issues since test suite reduction and prioritization are performed after mutation analysis.
Further, it includes a test input generation step that is not feasible in the context of CPS.
Finally, it has never been implemented and therefore its feasibility has not been evaluated.}
