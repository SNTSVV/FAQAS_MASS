% !TEX root = MAIN.tex

\chapter{Toolset Limitations and Open Problems}
\label{sec:limitations}

FAQAS led to developing tools that enable the application of mutation analysis and testing to space software. FAQAS mainly focused on selecting and extending existing preliminary research techniques to be applied in the space context. Also, FAQAS led to guidelines for the adoption of mutation analysis and mutation testing strategies within ECSS activities. However, FAQAS did not address some mutation testing and analysis problems that are out of the scope of the project or can be considered open research problems: 
\begin{enumerate}
\item FAQAS does not include methods to support an efficient integration of mutation analysis and testing techniques within software development and validation processes, which was out of the scope of FAQAS. Also, because of the focus on existing methods, FAQAS did not address problems that are partially discussed in the literature, such as 
\item FAQAS cannot generate high-level mutants representative of errors in the understanding of software specifications;
\item The FAQAS test generation approaches require a great deal of manual effort for their configuration.
\end{enumerate}

\section*{FAQAS Limitation 1 – Limited integration with software development practices}

The integration of FAQAS mutation analysis techniques within space software development practices is currently limited by two factors: the lack of guidelines to identify a satisfactory mutation score and the lack of a strategy for the inspection of mutants. Procedures to determine an acceptable mutation score are necessary because it is unlikely to achieve a 100\% mutation score. Indeed, the generation of new test cases to increase the mutation score has an associated cost, which prevents achieving a 100\% mutation score. Test generation cost depends on multiple factors. First, when automated test generation techniques are adopted, the selection of inputs remains expensive (e.g., it requires computation time and manual setup of tools). Second, in the presence of equivalent mutants (which range between 2\% and 69\% of the live mutants~\cite{Oscar:MASS:TSE}), test generation techniques simply indicate to the end-user that the inputs to kill the mutants had not been found (determining if a mutant is equivalent to the original program is an undecidable problem~\cite{papadakis2019mutation}); consequently, the engineer shall manually inspect the mutant to determine if such inputs might exist. The manual inspection of mutants is time-consuming and, therefore, expensive. Since it is unlikely to achieve a 100\% mutation score, defining guidelines that enable engineers to determine when it is acceptable to stop improving a test suite is necessary. Also, to adopt mutation analysis within ISVV practices, ISVV officers need similar guidelines to decide when a software test suite shall be considered adequate. Such guidelines may consist of threshold values to ensure different quality objectives. 

Also, it shall be desirable to prioritize and reduce the live mutants to be inspected. For example, it shall be reasonable to maximize the diversity among the mutants to be inspected first thus increasing the likelihood of identifying different test suite shortcomings by inspecting a limited set of mutants. Also, since mutants may be redundant with each other, implementing a set of test cases that kill a diverse set of mutants shall likely enable the discovery of other mutants not selected for inspection --- thus further increasing the mutation score. Identifying a diverse set of problems with a limited number of mutants to be inspected, would also enable ISVV officers to have a broad understanding of the problems affecting the test suite and help them evaluating the analyzed software artifacts.

\section*{FAQAS Limitation 2 – Lack of high-level mutants}

The validation of FAQAS results performed by industry partners has shown that, frequently, the errors introduced by mutation operators are fine-grained corner cases, for which the implementation of a dedicated test case may appear necessary to software engineers. Indeed, under the assumption that software engineers are reliable and avoid simplistic mistakes, some of the faults injected by code-driven mutation operators (e.g., forgetting a clause in one expression) appear pessimistic and of little relevance to assessing the quality of test suites. Data-driven mutation analysis partially addresses the problem by introducing errors at a higher level (i.e., by altering the data exchanged by components); however, data-driven mutation analysis suffers from the opposite problem, the faults it simulates are too coarse (e.g., values out of range) and test suites likely discover them. In practice, the literature lacks mutation operators that simulate specification-related errors (e.g., incorrect understanding of a sentence in the requirement specifications). We believe it is thus necessary to develop methods to drive mutation analysis based on software specifications. Specification-driven mutation analysis can be achieved in multiple ways. For example, for code-driven mutation analysis, it might be necessary to identify mutants that may reflect the misunderstanding of a requirement or the introduction of an articulate fault concerning multiple statements. Recent advances in higher-order mutation analysis~\cite{Wong:Higher} and natural language processing applied to program analysis might be an asset~\cite{Mai:NLP}. For data-driven mutation analysis, to increase its effectiveness, it is necessary to augment fault models with additional information about the system, for example, state-related data constraints. To this end, data-driven mutation analysis shall be combined with model-based testing artifacts (e.g., statecharts or timed automata).

\section*{FAQAS Limitation 3 – Limited test generation effectiveness with floating point instructions and external components}

The FAQAS code-driven mutation testing solution (i.e., the SEMuS tool) is limited by some peculiarities of KLEE, the state-of-the-art test generation tool integrated into SEMuS. Indeed, KLEE lacks support for floating-point computation and external, loosely coupled components. Also, FAQAS lacks a solution for data-driven mutation testing. Unfortunately, these limitations are open research problems for which industry-ready solutions do not exist. We briefly describe these problems in the following paragraphs. First, KLEE does not properly support floating-point arithmetic; consequently, it cannot generate test cases killing the mutants for most mathematical functions used by space software. Prototype extensions of KLEE working with floating-point arithmetic support exist~\cite{Liew,Liew:Fuzz}; however, their applicability to space software — and cyber-physical software more in general — remain to be assessed. Second, KLEE’s limitations concerning the processing of external, loosely coupled components derive from the symbolic execution process it relies on. Indeed, to derive the test inputs that kill a mutant, KLEE generates a symbolic formula that includes all the assignments observed in an execution path taken by the software under test. The formula is passed to a constraint solver (e.g., Z3~\cite{Z3C}), which looks for variables’ assignments that satisfy all the constraints in the formula (e.g., path conditions and assertions). In the presence of external libraries (e.g., calls to network functions or components without source code), KLEE cannot derive a formula that captures how the software computes its results; consequently, it may not be able to identify the desired inputs. Since data-driven mutants target the communication between external components, symbolic execution is inadequate for data-driven mutation testing too. Concolic execution (i.e., relying on concrete inputs collected during testing and symbolic inputs to be identified through constraint solving) is a solution that may help addressing the limitations of symbolic execution; however, for mutation testing~\cite{chekam2021killing}, it has been evaluated only when testing opensource, bash programs, while its feasibility with large space software remains to be explored. Other valid alternatives are model-based test generation approaches, which are not affected by the limitations of static program analysis and symbolic execution~\cite{di2017augmenting}. 

\section*{FAQAS Limitation 4 – Limited test generation efficiency}

Another limitation of the FAQAS mutation testing solution is the need for the manual inspection of the generated test cases. More precisely, SEMuS, the FAQAS tool for mutation testing, requires engineers to verify that structured inputs and oracles are correctly set. This need for manual inspection depends on the nature of the targeted software (C and C++), which prevents the automated identification of input and output variables (e.g., a pointer variable might be used to provide input and outputs). FAQAS2 shall explore to possibility of relying on the available manually written test suites to automatically extract (e.g., through static program analysis) all the information required to generate executable test cases.

\section*{Future developments}

The above-mentioned limitations, might be addressed by a follow-up activity having the following objectives:

\begin{itemize} 
\item \EMPH{Objective 1: Support the integration of mutation analysis into space software development practices.}
The activity shall develop a toolset that (1) determines thresholds for the mutation score that provide software quality guarantees (e.g., absence of severe failures, reduction of field failures, increased fault detection) and (2) select representative mutants to be inspected by engineers. The target users for the toolset shall be both engineers for space software companies and officers performing ISVV.

\item \EMPH{Objective 2: Specification-driven mutation analysis.} The activity shall extend code-driven mutation operators to generate higher-order errors, possibly simulating mistakes in the understanding of software specifications. Also, it shall extend data-driven mutation analysis with model-based support (e.g., to capture valid data ranges for specific program states).

\item \EMPH{Objective 3: Improve automated test generation effectiveness and efficiency.} The activity shall overcome the limitations of the test case generation tools used in FAQAS. To this end, the activity shall evaluate solutions available in the literature (e.g., to address floating point limitations) and leverage characteristics typical for the mutation testing context like the availability of seeds (e.g., the test cases that cover the mutated code without killing the mutant).

\end{itemize} 

