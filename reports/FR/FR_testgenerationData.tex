% !TEX root = Main.tex

\section{Data-driven Mutation Testing}
\label{sec:testGenerationData}



Although the literature does not address the problem of automatically generating test cases for data mutation testing, in this section we provide references to work that can be reused for this purpose. 
We discuss both the generation of \INDEX{test inputs} and \INDEX{test oracles}. 
Concerning the generation of test inputs, we group the applicable approaches according to the type of models used to specify the data to be generated: \INDEX{UML models}, \INDEX{grammars}, \INDEX{block models}, and \INDEX{no models}. 

Automated test inputs generation aims to automatically generate data that can be altered through a mutation operator. 
In the context of system and integration testing, which is the target of data-driven mutation testing, test input data is provided through input interfaces. However, since data-driven mutation is applied to data exchanged different communication layers, including input interfaces and interfaces used for the communication among internal components, we may observe two possible situations. First, the data targeted by mutation testing coincide with the input data (i.e., it was not transformed internally). Second, the data targeted by data mutation is the result of a transformation of the input data. We thus need to consider both the two cases when discussing related work.

\subsection{UML models}
%model-based fuzzying and 

%In the case of modelling based on UML models, a data mutation operator may not be applied during mutation testing when the type of data that it is supposed to alter is not observed during the execution of the test cases. 
%For example, in the case of the data model in Figure~\ref{fig:dataModel}, we may not be able to apply the operator \emph{Attribute Replacement with Random} to the attribute \emph{destinationId} of class \emph{GpsrPacketHeader} if all the test cases contain messages with \emph{SarPacketHeaders}.



When the data exchanged on the communication layer targeted by mutation testing coincide with the input data, existing approaches based on constraint-solvers can be adopted. 
These approaches rely on a formal or semi-formal specification of the structure of the input data and the constraints among data fields.
Some techniques target the generation of inputs whose data structure had been specified using a UML class diagram where the relations among data fields have been captured using OCL constraints. The data structure model is a UML class diagram and resembles the one reported in Figure~\ref{fig:dataModel}. The OCL language is instead used to capture all the constraints among data fields. Existing techniques in this category work by generating class diagram instances that satisfy a set of given OCL constraints by executing appropriate constraint solvers after having transformed the OCL constraints into other formalisms such as \INDEX{Alloy models}~\cite{Uml2alloy}, \INDEX{constraint satisfaction}~\cite{EMFTOCSP}, \INDEX{SMT}~\cite{Przigoda2016}, or \INDEX{SAT} problems~\cite{Soeken2011}. 

Other approaches, instead, work with models specified in formats other than UML class diagrams:
\INDEX{Java classes}~\cite{Boyapati-KORAT-ISSTA-2002,gligoric2010test}, \INDEX{constraint logic}~\cite{Senni-CPLgeneration-TAP-2012}, \INDEX{Alloy}~\cite{Khurshid-SpecificationBasedTesting-ASE-2004}, or \INDEX{Z specifications}~\cite{Horcher-Z-1995}.
These techniques have been proven to be effective for testing software systems that process classical data structures like trees. 
Alloy is a modelling language for expressing complex structural constraints~\cite{Jackson:Alloy:2002},
which has been successfully used to generate test inputs for testing object-oriented programs~\cite{Khurshid-SpecificationBasedTesting-ASE-2004}.
Korat, instead, is a technique that enables the generation of data structures to test Java programs~\cite{Boyapati-KORAT-ISSTA-2002}. Given a bound to the input structures (i.e., the maximum number of instances for each class to be used), Korat exhaustively generates all the nonisomorphic structures that are valid. 
Some of the limitations of Korat include requiring the definition of an imperative predicate that evaluates the correctness of the generated structure, which could be complex in the case of complex data models, requiring the manual definition of an input bound for each non primitive attribute or association, which might be particularly expensive in case of complex data structure,  not dealing with constraints defined over integers.
A more efficient, black-box test generation approach is UDITA~\cite{gligoric2010test}.
What contributes to the efficiency of UDITA is the combination of both generator methods and predicates.
Generator methods are used to build instances of the data structure, while predicates are used to validate the generated instances. UDITA relies upon the Java Path Finder model checker~\cite{Visser-JPF-2004} to generate all the instances that satisfy the given predicates. However, the implementation of these generator methods that define the complete structure of a complex data model instance and lead to realistic test inputs can be quite expensive. 

A common limitation of solutions based on constraint-solvers is their scalability~\cite{di2017augmenting}. In the context of mutation testing, where existing test inputs are available, a possible solution to address the scalability problem may consist of generating new test inputs by \EMPH{regenerating only portions of existing test inputs}.
For example, Di Nardo et al.~\cite{di2017augmenting} automatically generate test inputs for new requirements by adapting existing field data.
This is achieved by combining model transformations with constraint solving.
%% as shown in Fig.~\ref{fig:DiNardo}.
%Model transformations enable the partial reuse of existing field data, while constraint solving allows for the generation of missing data. The approach requires a model of the structure of the data provided by using an UML class diagram with OCL expressions capturing the constraints among data fields. In the case of Di Nardo et al., the missing data correspond to new data types and constraints introduced after updating the requirements of the software under test. In the context of mutation testing, these may correspond to data types not observed during the execution of the test suite.
%In Step 1, a chunk of data is loaded in memory as an instance of the original data model (Original Model Instance).
%A dedicated parser is used for that purpose.
%In Step 2, a model transformation applied to the Original Model Instance is used to generate an instance of the Updated Data Model. The result is an instance of the Updated Data Model that is incomplete (Incomplete Model Instance).
%It contains only the information that can be directly derived from the Original Model Instance:
%instances of classes and attributes that have been introduced in the Updated Data Model are missing from the Incomplete Model Instance (these instances are the ones generated in the next steps of the algorithm).
%In Step 3, UML2Alloy~\cite{Uml2alloy} is used to generate an Alloy model that corresponds to the class diagram and the OCL constraints of the data model; the Alloy Analyzer~\cite{AlloyWeb} is then used to generate  valid instances of the updated data model by means of constraint solving. 
%Finally, in Step 4, to generate the concrete test inputs to be processed by the software under test (e.g. a binary file), the content of the Valid Model Instance is written in the format processed by the software under test through a dedicated encoder.
Despite empirical results show a huge performance gain with respect to traditional constraint-based approaches, the need for dedicated parsers to translate existing test inputs into class diagram instances may limit the applicability of the approach. Also, the available test inputs may not enable the generation of all the inputs data needed.

%\begin{figure}[t!]
%  \centering
%    \includegraphics{images/DiNardoTOSEM}
%      \caption{Automatic generation of test inputs for new data requirements proposed by Di Nardo et al~\cite{di2017augmenting}.}
%      \label{fig:DiNardo}
%\end{figure}

Other approaches address the scalability problem by relying on \INDEX{hybrid input generation approaches}~\cite{soltana2019practical}.
For example, PLEDGE~\cite{soltana2019practical} combines metaheuristic search and Satisfiability Modulo Theories (SMT~\cite{SMT:2011}) to generate UML instance models from UML class diagrams annotated with OCL constraints. It  works by using the Negation Normal Form (NNF~\cite{NNF:2001}) to represent all the constraints derived from the UML data model. Different subformulas that build the NNF formula are then solved by combining metaheuristic search and SMT. Metaheuristic search is used to handle subformulas whose satisfaction involves structural tweaks to the instance model, i.e., additions and deletions of objects and links. SMT is used with subformulas involving only primitive attributes, i.e., attributes with primitive types. 


When the data exchanged on the communication layer targeted by mutation is the result of a transformation of the input data, the only applicable solution consist in the application of approaches that generate inputs from scratch. By generating a large number of inputs that include instances for all the classes of the data model, these approaches may, in principle, lead to the generation of required data by internal components. However, without means to drive the generation of such data, the application of UML-based test input generation approaches in these situation is likely to be inefficient.

%Despite this approach enables efficient generation of test inputs from scratch, it may still require dedicated parsers for the con

\subsection{Grammars}

When grammars are used to model the input, \INDEX{grammar-based test input generation} approaches relying on the expansion of the production rules of the grammar can be adopted~\cite{fuzzingbook2019:GrammarFuzzer}. 
Available tools are shown in Table~\ref{table:grammarGeneration}.

%Listing~\ref{TestJSON} shows the inputs generated by \emph{Coverage-based Fuzzer}~\cite{fuzzingbook2019:GrammarFuzzer} to cover all the production rules and terminals of the grammar (one input per line). Listing~\ref{TestJSON} shows that these automatically generated inputs are difficult to read. For example, for a human, is more difficult to  read Listing~\ref{TestJSON}  than Listing~\ref{JSONfile}.
%
%\input{listings/testJSON.tex}

\input{tables/table_grammar_based_testing.tex}

Among existing approaches, \INDEX{Parser-Directed Fuzzing} (hereafter, \emph{pFuzzer}) aims at producing valid inputs for input parsers~\cite{mathis2019parser}. The challenge is to cover all the lexical and syntactical features of a certain language. The approach systematically produces inputs for the parser and tracks all the comparisons made; after every rejection, it satisfies the comparisons leading to rejections, effectively covering the input space. 
Evaluated on five subjects, from CSV files to JavaScript, the \emph{pFuzzer} prototype covers more tokens than both lexical-based (AFL) and constraint-based approaches (KLEE).


Similarly to the case of UML-based test case generation approaches that generate inputs from scratch, these grammar-based approaches can be adopted when the communication layer targeted by mutation testing is either the input layer or an internal layer.However, they may suffer of inefficiency problems; in additions, grammar can be unlikely used to model the types of data processed by space software.

%\cite{grammar:survey}

\subsection{Block-models}

Among the existing toolsets based on block-models, \INDEX{Peach} is the only one which supports the automated generation of test data. However, it's implementation simply generates random data, the process is referred to as 
\INDEX{blind fuzzing}\footnote{https://wiki.mozilla.org/Security/Fuzzing/Peach$\#$Creating\_a\_Data\_Model}.


\subsection{No models}


The generation of test inputs without the need of data model is typically driven by code coverage. A representative solution is given by AFL, which has been introduced in Section~\ref{sec:data_operators}. 

More in general, approaches that address the problem of automatically testing programs that process structured data can be adopted for this purpose~\cite{Kiran:2008,Braione:2017,Braione:2018}. For example, SUSHI~\cite{Braione:2018} is a tool that aims to cover test objectives that depend on non-trivial data structure instances. 
It relies on symbolic execution to generate path conditions that capture the relationship between program paths and input data structures. The path conditions are then translated into fitness functions to enable testing based on meta-heuristic search. A solution for the search problem is a sequence of method invocations that instantiates the structured inputs to exercise the program paths identified by the path condition. 

\subsection{Automated Generation of Test Oracles}
\label{sec:oracles:dataMutation}

In the case of data-driven mutation testing, solutions for the \INDEX{automated generation of oracles} that consist of assertions verifying the output of the software under test (see Section~\ref{sec:oraclesGeneration:codeDriven}) might still be applied. However, considering that data-driven mutation testing is more likely adopted in the context of system-level testing, where inputs consist of complex, structured data, the generation of test oracles using techniques built for unit testing is likely infeasible in this context. 

Possible solutions may consist of approaches that verify the correctness of the log files generated during the execution of the program~\cite{Pastore:ISSRE:08,Pastore:TKT:17}.
Given a set of log files (or execution traces) generated during valid executions, these approaches can derive \INDEX{finite state automata} (FSAs) that capture the sequences of events and data-flow observed in valid executions. The derived FSAs can be used to verify if new executions match the inferred models. More precisely, they enable the automated detection of invalid sequences of events and data-flows. Despite none of these approaches had been applied in the context of data-driven mutation testing, traces collected from multiple runs of a same test case might be used to derive an FSA that captures the behaviour of a single test case. By relying on multiple traces collected from a same test case we can leverage the generalization power of the inference engines used to generate the FSAs; these inference engines, for example, can automatically recognize and filter out variable elements such as timestamps. The inferred FSAs could thus then be used as oracles for newer executions of the generated test cases. Such approaches have shown to be effective in detecting both functional failures~\cite{Pastore:ISSRE:08} and performance problems~\cite{Pastore:TKT:17}.



