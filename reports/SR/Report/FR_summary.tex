% !TEX root = MAIN.tex


\section{Empirical evaluation}
\label{sec:summary:results}

The FAQAS activity has ben evaluated through an extended empirical evaluation; below we summarize our findings. 


\subsection{MASS}

\begin{table}[htb]
\caption{Code-driven mutation analysis results: MASS mutation score.}
\label{table:results:mass} 
\small
\centering
\begin{tabular}{|
>{\arraybackslash}p{54mm}@{\hspace{1pt}}|
>{\raggedleft\arraybackslash}p{40mm}@{\hspace{1pt}}|
}
\hline
\textbf{Subject}&\textbf{MASS Mutation Score (\%)}\\
\hline

\SAIL{}$_{S}$ (System test suite)&65.95\\

\SAIL{}$_{S}$ (Unit+System test suite)&70.56\\

\GCSP{}&70.92\\
\PARAM{}&85.95\\

\UTIL{}&84.41\\
\MLFS{}{}&93.49\\
% K: 3104 L: 2219-1480=739 T: 5323-1480=3843
ASN1SCC&80.77\\
\hline
$\textbf{Average}$&78.86\\
\hline
\end{tabular}

\end{table}


Table~\ref{table:results:mass} provides the code-driven mutation analysis results. The mutation score computed by \MASS for the case study subjects considered in our experiments was in line with the expectations of engineers. Both GSL and LXS have manually inspected a subset of the live mutants identified by \MASS (18 for \UTIL, 7 for \GCSP, 9 for \PARAM, 19 for \ESAIL). The inspection enabled industry partner to identify relevant shortcomings in their test suites: 
\begin{itemize}
\item 30 live mutants were due to missing inputs (7 for \UTIL, 3 for \GCSP, 9 for \PARAM, 11 for \ESAIL). In this cases engineers need to implement additional test cases that exercise the SUT with inputs not considered in the test suite. Of particular relevance are exceptional cases not being coverage (e.g., test suite was exercising the case of an error overflow 
caused by the parameter ‘value’ for a hash table but was not exercising the case of an error overflow caused by the parameter ‘key’).
\item 12 live mutants were due to missing oracles (2 for \UTIL, 4 for \GCSP, 6 for \ESAIL). In one case there was a missing oracle to verifies the correct encryption of all blocks encrypted by a certain function. In other cases the test suite was not verifying the output of the commands sent to the ADCS because verified when testing with hardware in the loop.
\item one fault was detected.
\item only two live mutants had ben reported (for \ESAIL) and only eight mutants not relevant because concerning third party software (for \UTIL).
\end{itemize}

%In addition, based on an independent evaluation performed on a case study subject not shared with the FAQAS team, LXS has reported that 36\% of the 34 live mutants detected by \MASS spot major limitations of the test suite.
%In their independent evaluation with libraries, industry partners did not negatively comment about the scalability of the process. However, they reported the need for a strategy to further prioritize the generated mutants for inspection.


Finally, our results show that such an optimized solution helps address scalability problems to a significant extent by reducing mutation analysis time by more than 70\% across subjects. 
Also, our results show that our sampling approach still lead to an accurate estimation of the mutation score. 
In practice, for large software systems like \SAIL{}\emph{-CSW}, such reduction can make mutation analysis practically feasible; indeed, with 100 HPC nodes available for computation, \APPR can perform the mutation analysis of \SAIL{}\emph{-CSW} in half a day. In contrast, a traditional mutation analysis approach would take more than 100 days, thus largely delaying the development and quality assurance processes.



\subsection{SEMuS}

\begin{table}[htb]
\caption{Test suite augmentation results.}
\label{table:results:test-gen} 
\centering
\footnotesize
\begin{tabular}{|
@{\hspace{1pt}}p{10mm}|
@{\hspace{1pt}}>{\raggedleft\arraybackslash}p{18mm}@{\hspace{1pt}}|
>{\raggedleft\arraybackslash}p{35mm}@{\hspace{1pt}}|
>{\raggedleft\arraybackslash}p{25mm}@{\hspace{1pt}}|
 >{\raggedleft\arraybackslash}p{25mm}@{\hspace{1pt}}|
}
\hline
\textbf{Subject}&\textbf{Live Mutants}&\textbf{Additionally Killed Mutants}&\textbf{Original MS (\%)}&\textbf{Updated MS (\%)}\\ 
\hline
$\mathit{MLFS}$&3\,891&697&81.80&85.06\\
$\mathit{ASN.1}$&2\,219&1\,729&58.31&90.79\\
$\mathit{ESAIL_S}$&1\,041&NA&70.56&NA\\
% additionally killed: clock 2 error 4 timestamp 6 memory 21
$\mathit{Libutil}$&4\,198&35&81.80&81.96\\
\hline
\end{tabular}

\end{table}

The empirical evaluation demonstrated the scalability of \SEMUS for the case study subjects in which it can be successfully applied (i.e., ASN1CC, MLFS, and \UTIL). However, it is currently limited by the choice of compiling with LLVM only the source file under test (to limit the probability of compilation errors). Table~\ref{table:results:test-gen} provides an overview of some of our results.

Our results also demonstrated the usefulness of \SEMUS. Indeed, \SEMUS enabled the identification of two fault in our case studies. Also, the generated test cases concerned inputs that are relevant (according to specifications) but not tested by the test suites.


\subsection{DAMAt}

\begin{table}[htb]
\caption{Data-driven mutation analysis results.}
\label{table:results:data-driven} 
\center
\footnotesize
\begin{tabular}{|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{24mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{12mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{12mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{18mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{12mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{12mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{12mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{12mm}@{\hspace{1pt}}|
@{\hspace{0pt}}>{\raggedleft\arraybackslash}p{12mm}@{\hspace{1pt}}|
}
\hline
\textbf{Subject} & 
\textbf{\# FMs} & 
\textbf{FMC} & 
\textbf{\#MOs-CFM} & 
\textbf{\#CMOs} & 
\textbf{MOC}  
&\textbf{Killed}&\textbf{Live}&\textbf{MS}
\\
\hline

\ADCS &10 &90.00\%   & 135 & 100 & 74.00\%   &    45&55&45.00\%\\
\GPS &1 &100.00\%    &  23  &  22 & 95.65\%    &      21&1&95.45\%\\
\PDHU &3 &100.00\%  &   29 & 24 & 82.76\%   &     24&0&100.00\%\\
\PARAM &6 &100.00\%  &   44 & 41 & 93.20\%  &        37&4&90.24\%\\
\GCSP &1 &100.00\%  &   33 & 21 & 63.64\%  &        NA&NA&NA\\


\hline

\end{tabular}

FM=Fault Model, FMC=Fault Model Coverage, MOs-CFM=Mutation Operations in covered FMs,
CMO=Covered Mutation Operation, MOC=Mutation Operation Coverage, Killed=Number of mutants killed by the test suite, Live=Number of mutants not killed by the test suite, MS=Mutation Score. The mutation score for \GCSP is not available because of nondeterminism observed while running the experiments.

\end{table}


Table~\ref{table:results:data-driven} shows the mutation analysis results of DAMAT.
The empirical evaluation with \ESAIL has demonstrated the effectiveness of the approach. Indeed, LXS has indicated that 57\% out of the overall amount of 102 test suite problems detected by DAMAt were spotting major limitations of the test suite. Also, GSL has confirmed that the approach enabled the detection of relevant test suite shortcomings.
One possible limitation of the approach is that it may introduce slow-downs that lead to non-deterministic failures when the test suite exercises brief interaction scenarios in which most of the operations performed concern the encapsulation of data into the network; this is what happened for the \GCSP case study subject.

Our results confirm that (1) uncovered fault models (i.e., low \INDEX{FMC}) indicate lack of coverage for certain message types (\INDEX{UMT}) and, in turn, the lack of coverage of a specific functionality (i.e., setting the pulse-width modulation in \ADCS); (2) uncovered mutation operations (i.e., low \INDEX{MOC}) highlight the lack of testing of input partitions (\INDEX{UIP}); (3) live mutants (i.e., low \INDEX{MS}) suggest poor oracle quality (\INDEX{POQ}).

Based on our evaluation, we observed that live mutants can be killed by introducing oracles that (1) verify additional entries in the log files (39 instances for \ADCS, 1 instance for \GPS), (2) verify additional observable state variables (14 instances for \ADCS, 4 instances for \PARAM), and (3) verify not only the presence of error messages but also their content (2 instances for \ADCS).
\REVOCT{C-P-18}{Such oracles may consist of additional assertions that verify data values already produced by the software under test (i.e., no modification of the SUT is needed).}

%Finally we also identified the following set of test suite shortcomings: (1) the test case does not distinguish failures across data items (e.g., temperature values collected by different sensors),
%(2) the test case does not distinguish errors across different messages (e.g., in \ADCS, the IfHK message reporting a broken sensor or the message sent by a sensor reporting malfunction), (3) the test case does not distinguish between errors in nominal and non-nominal data (e.g., it does not distinguish between VOR and FVOR), (4) the test case does not distinguish between upper and lower bounds (e.g., the mutants for VOR lead to the same assertion failures), and {(5) the test case does not distinguish between different error codes (i.e, it simply verifies that an error code is generated)}. 

\subsection{DAMTE}

\DAMTE aims to address a task (i.e., test generation at system and integration level) that is particularly difficult to address with state-of-the-art technology (e.g., test generation toolsets based on symbolic execution). In particular, the test generation adopted in FAQAS (i.e., KLEE) requires manual intervention to specify which are the inputs to select thus preventing the automated generation of a large number of test cases. For code-driven mutation testing we have addressed this issue by relying on a template generator, which is difficult to implement for data-driven test generation because the identification of the function to test is hardly to automate (given that data-driven mutation analysis targets integration and system testing, it might be either the function with the mutation probe or another one). Also, and more importantly, it requires the compilation of the whole software under analysis through LLVM, which is often not feasible. For the reasons above, it is not feasible at the current stage to automate test generation of the whole software under test; consequently, 
it has not been possible to perform a large scale evaluation of the approach but only to focus on its feasibility analysis.

We relied on DAMTE to generate inputs for the \PARAM client API functions. The invocation of the \PARAM client API functions with the identified inputs enables the definition of integration test cases that exchange messages between the \PARAM client and the \PARAM server. The exchanged messages include data item instances that enable the execution of mutation operations that were not covered in the DAMAt empirical evaluation.

Overall, we conclude that the DAMTE approach may be feasible; however, it requires some manual effort for the configuration and execution of test cases which may limit its usefulness. The first step towards its large scale applicability is the improvement of underlying test generation tools and compiler procedures, such changes will facilitate DAMTE application to large projects without the need for manually creating test template files with dependencies.

\subsection{Summary}

The developed toolset has thus demonstrated to be useful in industrial contexts. The common limitation cross the different tools is the usability; indeed, all the tools require relevant effort to be set-up (however, LXS has reported that if at least 6\% of the reported problems spot major limitations the benefits surmount costs). The need for manual effort mostly depends on the lack of a common development environment for different case study subjects. The identification of a reference platform for software development in industry context may facilitate the adoption of the FAQAS toolset.

Other limitations that need further research effort to simplify the adoption of the FAQAS toolset are the prioritization of mutants to be inspected, the need for a solution to compile whole SUTs with LLVM, the need for a solution to enable test generation in the presence of floating point variables, the need for a working solution to enable test generation based on data-driven mutation analysis results. 

