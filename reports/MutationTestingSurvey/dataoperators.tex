% !TEX root = MutationTestingSurvey.tex

\section{Data-driven Mutation Operators}
\label{sec:data_operators}

As mentioned in Section~\ref{sec:dataProcess}, the software engineering literature does not include any study on data-driven mutation testing but only testing approaches based on the injection of data faults.
For this reason, 
in this section we provide an overview of the fault injection techniques that have originally been developed to support software testing and can be used in the context of data-driven mutation testing. 
More precisely, we focus on the techniques for the modification of data that are presented in software testing research papers.
We refer to these techniques as \emph{data-driven mutation operators}.

Table~\ref{table:dataOperators} provides an overview of the data-driven mutation operators that can be applied to the case of space software and embedded systems. 
We selected the set of data-driven mutation operators in Table~\ref{table:dataOperators} based on the different type of faults commonly affecting space and embedded systems. 
Each operator aims to create data faults that might be observed in real systems either because of hardware errors or software errors.

In Table~\ref{table:dataOperators}, column \emph{Name} provides the name of the specific operator described,  
 column \emph{Type of fault} indicates the type of faults each operator aims to simulate,
 column \emph{Data model} indicates if the operator requires a model of the structure of the data to be mutated,
 column \emph{Definition} provides a brief description of the mutation operator, column \emph{Target} indicates the type of data targeted by the approach,
 column \emph{Reference} provides a reference to a research paper or tool describing the operator more in detail.
 
%Fabrizio: "First" and "Then" read like a story, which is not good in a Tech Report
Concerning the type of faults considered, we focus both on hardware and software errors.
For hardware errors, we include the categories \emph{CPU Faults}, \emph{Memory Faults}, and \emph{Signal Faults}. 
For software errors, we include the category of \emph{Data Processing Faults}.
Category \emph{Communication Faults} simulates problems that can be caused either by software or hardware errors.

In the following, we provide a brief overview of the operators generating each type of fault:
\begin{itemize}
	%Note: always put a comma at the end of a list before "or" "and"
	\item Category \emph{CPU Faults} includes operators that perform mutations in the contents of an individual bit, byte, or word in a CPU register. The mutations in this category can target saved, floating-point, program-counter, global and stack-pointer register locations. 
	\item Category \emph{Memory Faults} includes operators that perform mutations in the contents of an individual bit, byte, or word in a memory register. The mutations in this category can target stack, heap, global-data and user-defined memory locations.
	\item Category \emph{Communication Faults} includes operators that simulate packet corruption faults. The mutations in this category can target channels between components, single messages, and the addresses of the messages to be exchanged.
	\item Category \emph{Data Processing Faults} includes techniques that mutate the data being processed by the system. Some of these techniques are white-box, they process the source code of the application to generate inputs more efficiently. Other techniques require a model of the data to be mutated. The mutations in this category can target both the input and the output parameters of the interface of a software component.
\end{itemize}

\newcommand{\FTAPE}{\cite{tsai1999stress}}
\newcommand{\FIAT}{\cite{barton1990fault}}
\newcommand{\GOOFI}{\cite{aidemark2001goofi}}
\newcommand{\DOCTOR}{\cite{han1995doctor}}
\newcommand{\ORCHESTRA}{\cite{dawson1996testing}}
\newcommand{\Fuzz}{\cite{miller1995fuzz}}
\newcommand{\Ballista}{\cite{koopman2000exception}}
\newcommand{\RIDDLE}{\cite{ghosh1998testing}}
\newcommand{\Superion}{\cite{Wang:GrammarAwareFuzzying:ICSE:2019}}
\newcommand{\AFL}{\cite{gutmann2016fuzzing}}
\newcommand{\SAGE}{\cite{godefroid2012sage}}
\newcommand{\pFuzzer}{\cite{mathis2019parser}} 
\newcommand{\MoWF}{\cite{pham2016model}}
\newcommand{\DiNardoICST}{\cite{di2015generating}}
\newcommand{\DiNardoASE}{\cite{di2015evolutionary}}
\newcommand{\Matinnejad}{\cite{Matinnejad19}}
\newcommand{\MongoDB}{\cite{Guo:MongoDBFuzzer:CACM:2017}}
\newcommand{\SOLMI}{\cite{Jan:ISSTA:2016}}
\newcommand{\MUSQL}{\cite{Appelt:SQLI:ISSTA:2014}}

Table~\ref{table:dataMutation:references} provides the names of the tools and approaches referenced in Table~\ref{table:dataOperators}, along with a URL for the download of the tool, if available.

\input{tables/data_operators_references}

\input{tables/table_data_operators}

Among all the data mutation operators reported in Table~\ref{table:dataOperators}, the ones targeting \emph{Data Processing Faults} are more powerful since they concern the modification of complex data structures. %Oscar: not sure that "overviewed" is spelled right %They are briefly overviewed in the following.
We provide an overview in the following.




%\subsubsection{Approaches generating data from scratch}
\emph{AFL} is an instrumentation-guided fuzzer~\cite{gutmann2016fuzzing}. It integrates genetic algorithms with a form of branch coverage called state transition coverage,
which captures the number of times each branch is taken in an execution. 
AFL aims to generate inputs that exercise all the different code paths.
%It is implemented by executing for every instruction of the program under test the following lines
%cur_location = <COMPILE_TIME_RANDOM>;
%coverage[cur_location ^ prev_location]++; 
%prev_location = cur_location >> 1;
It requires one or more starting files that contain input data normally processed by the targeted application. 
Each provided input is processed in a queue. For each input in the queue, AFL trims the test case to the smallest size that doesn't alter the coverage of the program. Then it repeatedly mutates the file using a set of fuzzing strategies, which are presented in Table~\ref{table:AFL:operators}.
If any of the generated mutations result in a change of the coverage for the software under test, the mutated input is added to the input queue.

\input{tables/table_AFL_operators}

\emph{Superion}~\Superion~ extends \emph{AFL} to drive mutations based on grammars. It takes as input an ANTLR~\cite{ANTLR} grammar. 
It performs mutations that consist of \emph{grammar-aware trimming}, \emph{dictionary-based mutations}, and \emph{tree-based mutation}.
Grammar-aware trimming is performed by generating an AST tree from the input and by deleting a randomly-selected subtree. 
It aims to generate trimmed inputs that do not alter code coverage in the SUT.
Dictionary-based mutations are performed by building a dictionary using every token in the input file and by inserting the elements of the dictionary in the boundaries of every token in the input file.
Tree-based mutation is performed by processing two input files and replacing the subtrees of one file with the ones of another.
Listing~\ref{mutatedJSONfile} shows a JSON file generated by deleting a randomly-selected subtree from Listing~\ref{JSONfile}.

\input{listings/mutationJSON}

%Fabrizio: you had this definition of equivalent: "(i.e., correct fuzzed data, but semantically meaningless)." it is difficult to understand


%Fabrizio: I cannot understand the following sentences
%Although it tackles the problem of redundant mutants by well covering the space of possible inputs, it does not tackle the redundant mutant problem.
%Cannot handle semantic restrictions imposed by certain nontrivial input languages. The approach has difficulties to handle complex sequences recursion.

%\subsubsection{Approaches altering existing data}


\emph{$\mu$4SQLi}~\cite{Appelt:SQLI:ISSTA:2014} relies on a SQL grammar to generate SQL injections. 
SQL injections are generated by means of a set of mutation operators, shown in Table~\ref{table:Mu4SQLI}, that alter the values of an input according to predefined patterns.
Similarly, SOLMI~\cite{Jan:ISSTA:2016} relies on the XML grammar to alter XML messages based on a predefined set of mutation operators and introduce potential XML injections. 
SOLMI implements four operators that (1) introduce XML meta-characters, (2) introduce closing tags, (3) replicate XML elements, (4) replace XML content.


\begin{table}[h]
\caption{Mutation operators implemented by $\mu$4SQLi}
\label{table:Mu4SQLI}
\begin{tabular}{|p{2cm}|p{11.5cm}|}
\hline
\multicolumn{2}{|c|}{Behaviour-Changing Operators}\\
\hline
MO\_or&Adds an OR-clause to the input\\
MO\_and&Adds an AND-clause to the input\\
MO\_semi&Adds a semicolon followed by an additional SQL statement\\
\hline
\multicolumn{2}{|c|}{Syntax-Repairing Operators}\\
\hline
MO\_par&Appends a parenthesis to a valid input\\
MO\_cmt&Adds a comment command (- - or \#) to an input\\
MO\_qot&Adds a single or double quote to an input\\
\hline
\multicolumn{2}{|c|}{Obfuscation Operators}\\
\hline
MO\_wsp&Changes the encoding of whitespaces \\
MO\_chr&Changes the encoding of a character literal enclosed in quotes\\
MO\_html&Changes the encoding of an input to HTML entity encoding\\
MO\_per&Changes the encoding of an input to percentage encoding\\
MO\_bool&Rewrites a boolean expression while preserving it's truth value\\
MO\_keyw&Obfuscates SQL keywords by randomising the capitalisation and inserting comments\\
\hline
\end{tabular}
\end{table}



One of the major limitations of grammar-based approaches is the cost of defining an input grammar. MongoDB's javascript fuzzer addresses the problem of grammar-based mutation when a grammar is not available. Instead of mutating input files for the SUT based on the input grammar of  the SUT, it mutates test cases for the SUT (in this case the Javascript test cases)~\MongoDB. It replaces subtrees in the AST tree generated from the test cases either other subtrees belonging to the same input file or with subtrees generated following encoded production rules.




%, also, it requires more knowledge about the target than purely random ones.

\emph{SAGE} adopts symbolic execution to systematically generate malformed inputs~\cite{godefroid2012sage}. SAGE performs fuzzing on file- and packet-parsing applications. 
The program is first executed with concrete inputs; in order to identify a set of constraints on inputs, then, one of the constraints in the set is negated, and new malformed inputs are generated to satisfy the new set of constraints. 
The main benefit of SAGE is that it forces the program to execute corner cases not covered by the initial inputs; for example, 
%Fabrizio: you wrote the following, which I could not understand please check if my sentence is correct
%(e.g., reached one-third of all bugs found by fuzz testing in Microsoft projects \cite{bounimova2013billions}).
%Oscar: yes, what you wrote is correct 
one-third of all the bugs found by means of fuzz testing are detected thanks to SAGE \cite{bounimova2013billions}.
Unfortunately, the main limitation of SAGE and symbolic execution-based fuzzers is limited scalability, due to the high execution time required by symbolic execution.

%Fabrizio: not sure what the following is about, ignoring
%\emph{Testing of Fault-Tolerant and Real-Time Distributed Systems via Protocol Fault Injection \cite{dawson1996testing}}: The paper introduces a portable fault injection environment for testing implementations of distributed protocols.

%Fabrizio: we miss the pro/cons from the following
\emph{Model-Based Whitebox Fuzzing} targets program binaries that process structured inputs~\cite{pham2016model}. It efficiently generates valid inputs that exercise critical target locations. This is done through a directed path exploration technique that prunes from the search space those paths that are exercised by invalid, malformed inputs.
Compared with a Model-Based Blackbox Fuzzer (MoBF) approach, the \emph{MoWF} prototype was able to expose all of 13 vulnerabilities on an empirical evaluation carried on nine subject programs, while the MoBF approach only detected 6 out of 13 vulnerabilities. 



%\emph{Generating complex and faulty test data through model-based mutation analysis (Research paper) \cite{di2015generating}}: 
\emph{Model-based data mutation} concerns the automated generation of invalid input data through the mutation of existing data based on a predefined set of mutation operators~\cite{di2015generating}.
The technique receives two inputs: field data and a data model, i.e., a UML class diagram annotated with stereotypes and OCL constraints. 
An example data model has been shown in Figure~\ref{fig:dataModel} while an example OCL constraint appears in Figure~\ref{fig:costraint:firstHeader}. 
The technique relies upon six generic mutation operators to automatically generate faulty data. 
Table~\ref{table:dataModelMutationOperators} provides an overview of the mutation operators proposed in ~\cite{di2015generating}.
In model-based data mutation~\cite{di2015generating} stereotypes are used to tailor the behaviour of the generic mutation operators to the fault model for the system under test and the environment in which it is deployed. 
Table~\ref{table:faultModel:SES} shows a fault model for a satellite system that processes the data presented in Figure~\ref{fig:dataModel}.
Mutation operators are applied to the data according to the stereotypes used in the data model.
Table~\ref{table:mapping} shows the mutation operators and the corresponding stereotypes. In~\cite{di2015generating}, the mutation operator \emph{Attribute Bit Flipping} is applied on all the attributes not tagged with other stereotypes. 

\input{tables/table_stereotypes}

\input{tables/tableModelBasedOperators.tex}

\input{tables/table_faultModel.tex}

Data mutation may lead to the generation of inconsistent data containing trivial faults that do not comply with the given fault model (e.g., checksum errors). 
Inconsistent data might also be caused by mutation operators that target classes. For example the swapping of packets that belong to two different virtual channels may lead to the generation of VCDUs that contain packets with a same id, i.e. inconsistent data. To preserve data consistency the approach in \cite{di2015generating} enables software engineers to configure the behaviour of mutation operators by means of OCL queries and UML stereotypes. OCL queries are used to enable software engineers to further restrict the characteristics of the object instances on which the mutation operators can be applied.   The UML stereotype, \emph{Derived}, instead, enables software engineers to specify which attributes need to be updated after a mutation in order to prevent trivial errors. The stereotype requires that software engineers specify the name of a method that is invoked at runtime by the mutation framework to regenerate the value of the tagged attribute. The implementation of this function should be provided by the software engineer (e.g., a utility function named that recalculates the checksum of a packet). 



\emph{Search-based data mutation} relies on an evolutionary algorithm to perform model-based data mutation and optimize multiple objectives:  
cover all the classes of the data-model, cover all the possible faults of the fault model, cover all the clauses of the input/output constraints,
maximise code coverage~\cite{di2015evolutionary}.
The coverage of each objective is encoded by means of boolean arrays; this information is used to select a minimal set of inputs that maximize the coverage of the different objectives.
At every iteration, the evolutionary algorithm keeps only inputs that contribute to increase the coverage of at least one of the objectives (e.g., inputs that cover one instruction not covered by other inputs).


Most of the approaches presented in table Table~\ref{table:dataOperators} work by altering existing data, one exception is
\emph{Parser-Directed Fuzzing} (hereafter, \emph{pFuzzer}). Indeed, it aims at producing valid inputs for input parsers~\cite{mathis2019parser}. The challenge is to cover all the lexical and syntactical features of a certain language. The approach systematically produces inputs for the parser and tracks all the comparisons made; after every rejection, it satisfies the comparisons leading to rejections, effectively covering the input space. 
Evaluated on five subjects, from CSV files to JavaScript, the \emph{pFuzzer} prototype covers more tokens than both lexical-based (AFL) and constraint-based approaches (KLEE).
An additional source of information concerning the adoption of fuzzying and grammar-based approaches to perform test input generation is the \emph{Fuzzing Book}~\cite{fuzzingbook2019:GrammarFuzzer}.

\clearpage