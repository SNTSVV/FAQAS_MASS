% !TEX root = MutationTestingSurvey.tex

\chapter{Mutation Testing Benchmarks}
\label{chapter:industry}

\DONE{I removed the sections because useless.}

%\section{Mutation Testing Benchmarks}

\DONE{I removed the first paragraph. A little too wordy. The limitations do not concern only scalability.}
%In Section \ref{sec:limitations} we presented the limitations, and concretely the scalability issues of the mutation testing process. We also explained that the scalability issues of the mutation testing process are directly related to the size of the system under test and the respective test suite used to assess its adequacy. 
%
%To cope with the limitations of the mutation testing process, in Section \ref{sec:limitations} we also introduced several approaches and strategies present in the scientific literature. 

In Section \ref{sec:limitations} we have provided an overview of state-of the-art solutions to perform mutation testing and address limitations of the mutation testing process.
Each of these solutions had been evaluated against a set of case study systems deemed representative for the usage context. Such case study systems consist of a software under test (SUT) and a test suite for the software.
Hereafter, we'll use the term benchmark to indicate the set of case study systems used in the empirical evaluation presented in a research paper.
%introduce an empirical evaluation aimed to prove or disprove a specific theory proposed by the authors. 
These benchmarks can be used as reference for future mutation testing developments. 
Also, a deep understanding of the characteristics of the benchmarks considered in the literature, may provide insights on the generalizability of the results to space software (e.g, results evaluated against a real time application are more likely generalizable to the case of space software).

\DONE{most important = > top}
\TODO{check the date}

In this chapter we provide a survey of the benchmarks considered in empirical evaluations described in research work presented in top software engineering conferences and journals, between 2012 and 2020. Table~\ref{} provides the list of conferences and journals considered in our survey. For every venue, we have considered all the work evaluated against C and C++ software.

\TODO{Add the table of conferences, describe the columns in the text. Columns should be: name, type of venue (conference proceedings,journal),publisher.}

%Since the size of these bechmarks (e.g., under test and test suite are important to prove the validity of the proposed approach on real life software (e.g., space context software), in this chapter we present a benchmark of the most relevant empirical evaluations performed in the last eight years, collected from most important software engineering venues and journals (e.g., ICSE, ICST, ISSTA, ISSRE, AST, TSE and TOSEM), in the context of mutation testing process for C software. Section \ref{sec:benchmark} introduces a summary of the benchmark with the respective analysis.

%\section{Benchmarks Present in the Literature}
%\label{sec:benchmark}

\DONE{I rewrote, I do not expect you to introduce a table. You may have writte someting like "In this section we describe of the most relevant benchmarks targeting C software systems, they are summarized in Table..". I rewrote differently.}
%In the following, we introduce Table \ref{table:benchmarks}, the table shows a summary of the most relevant benchmarks targeting C software systems, used in the software engineering literature in the last eight years. 

\TODO{I think we should have two tables. One for the single case studies and one for the benchmark. I call you.}

Table XX provides the list of the case study systems considered in the literature. COlumn....

\TODO{We should have a column counting the number of papers using the tool}

Table \ref{table:benchmarks} shows the most used case study for assessing C software systems is the Siemens suite.
\TODO{You have to describe what types of programs it contains, when it was developed.}
The programs belonging to this suite are commonly used to evaluate state-of-the-art solutions because they it includes faulty versions affected by faults introduced by engineers during development.


Table provides the list of benchmark identified in the literature, we report (1) the case study (i.e., the software under test - SUT), (2) the size of the system being analyzed in terms of lines of code (LOC), (3) the size of the SUT test suite in terms of number of test cases, (4) the original goal of the evaluation, and (5) the actual reference to the paper in which the benchmark was originally presented. 

\DONE{"Appreciate" is a little too much :)}
%From Table \ref{table:benchmarks}, it also can be appreciated the wide variety of case studies selected in the literature, while some authors decide to assess their techniques using very simple programs such as \textit{abs} \cite{tokumoto2016muvm}. Other authors preferred using Unix applications such as the Coreutils package \cite{hariri2019comparing,papadakis2018mutation,chekam2017empirical}. 
%On the other hand, some authors selected very complex programs such as OpenSSL and LLVM \cite{denisov2018mull}, but usually the experiments are exercised only on small components of these large applications.

The dimension of the benchmark case studies vary a lot. They include simple programs such as \textit{abs} \cite{tokumoto2016muvm}, large Unix utilities such as the Coreutils package \cite{hariri2019comparing,papadakis2018mutation,chekam2017empirical}, and programs implementing complex functions such as OpenSSL and LLVM \cite{denisov2018mull}.
However, when large and complex software systems are considered in the empirical evaluation, the evaluation concerns only a subset of the components of these large applications.
For example... 
\TODO{Provide an example. Describe what they did in a paper where they selected a component, otherwise the sentence above might not be understood}

Concerning the size of the selected softwares under test, the most relevant studies has been presented by Papadakis and Chekam \cite{papadakis2018mutation,chekam2017empirical,papadakis2018mutant}. Specifically, these authors employed the case studies Coreutils, Findutils, Grep, Make and Codeflaws (83 KLOC, 18 KLOC, 9 KLOC, 35 KLOC and 266 KLOC respectively). 
Concerning the size of the test suite employed to assess their adequacy, again Papadakis and Chekam \cite{papadakis2018mutation,chekam2017empirical,papadakis2018mutant} present the most comprehensive studies with test suite sizes ranging from 58 131, to 122 261 test cases.

\TODO{Can we give a final message?}

\input{tables/industry}



