% !TEX root = MutationTestingSurvey.tex

\chapter{Mutation Testing Benchmarks}
\label{chapter:industry}

In Section~\ref{sec:limitations} we have provided an overview of state-of the-art solutions to perform mutation testing and address limitations of the mutation testing process.
Each of these solutions had been evaluated against a set of case study systems deemed representative for the usage context. Such case study systems consist of a software under test (SUT) and a test suite for the software.
Hereafter, we'll use the term \INDEX{benchmark} to indicate the set of case study systems used in the empirical evaluation presented in a research paper.
%introduce an empirical evaluation aimed to prove or disprove a specific theory proposed by the authors. 
These benchmarks can be used as a reference for future mutation testing developments. 
Also, a deep understanding of the characteristics of the benchmarks considered in the literature, may provide insights on the generalizability of the results to space software (e.g, results evaluated against a real time application are more likely generalizable to the case of space software).

%\DONE{Add the table of conferences, describe the columns in the text. Columns should be: name, type of venue (conference proceedings,journal),publisher.}

%\DONE{I rewrote, I do not expect you to introduce a table. You may have writte someting like "In this section we describe of the most relevant benchmarks targeting C software systems, they are summarized in Table..". I rewrote differently.}

%\DONE{You cannot say "in the followng we introduce a section." Either you write in the following we explain <something> or Section x presents.}
%\REVTWO{C40}{In the following, we introduce Sections~\ref{section:industry:code} and~\ref{section:industry:data}, which present detailed information about benchmarks on code-driven and data-driven mutation testing, respectively.}
\REVTWO{C40}{Sections~\ref{section:industry:code} and~\ref{section:industry:data} present detailed information about benchmarks on code-driven and data-driven mutation testing, respectively.}

%\DONE{check the date}

\section{Code-Driven Mutation Testing Benchmarks}
\label{section:industry:code}


%\DONE{If in teh case of data-driven we have used a different criterion (I think it is the case), you have to move the following paragraph to the section on code driven}



In this section we provide a survey of the benchmarks considered in empirical evaluations described in research work presented in top software engineering conferences and journals, between 2013 and 2019. Table~\ref{table:papers} provides the list of conferences and journals considered in our survey. For every venue, we have considered all the empirical evaluations considering C and C++ case study systems. Table~\ref{table:papers} provides the acronym and name of the venue, the type of venue, i.e., whether if it is a journal of a conference proceedings, and finally the publisher of the journal article or conference proceedings. 

\input{tables/papers}

\input{tables/case_studies}

Table~\ref{table:case_studies} provides the list of the case study systems considered in the literature. For each case study we report (1) the case study (i.e., the software under test - SUT), (2) the size of the SUT (the size may vary according to the specific study), (3) the size of the test suite for the SUT\footnote{The size may vary from an empirical evaluation to another; also, in some cases the test suite details were not available (N/A).}, (4) the number of papers that report empirical results with the provided case study, and (5) the references to the papers that rely on the case study.

%\DONE{You have to describe what types of programs it contains, when it was developed.}

Table~\ref{table:case_studies} shows that Siemens, Make and Space are the most common case studies with 6, 5 and 4 uses, respectively. 
The most used case study is the \INDEX{Siemens suite}. The programs belonging to this suite are commonly used to evaluate state-of-the-art solutions because the Siemens suite includes faulty versions affected by faults introduced by engineers during development. The suite is available through the Subject Infrastructure Repository (SIR) from the University of Nebraska-Lincoln\footnote{https://sir.csc.ncsu.edu/portal/index.php}. The Siemens suite contains a diverse collection of C programs that include code involving integer and floating-point operations, pointers, memory allocation, loops and complex conditional expressions. The Siemens suite was introduced in 1994 by Hutchins et al.~\cite{hutchins1994experiments}, the programs from the suite come with a large pool of test cases written initially by Hutchins et al. and then augmented by Rothermel et al.~\cite{rothermel1998empirical}.

Table~\ref{table:benchmarks} provides the list of benchmarks identified in the literature. We report (1) the case studies of the benchmark (i.e., the software under test - SUT), (2) the size of SUT in terms of lines of code (LOC), (3) the size of the SUT test suite in terms of number of test cases, (4) the original goal of the evaluation, and (5) the actual reference to the paper in which the benchmark was originally presented. \REVTWO{C39}{Not all the referenced papers explicitly mention the type of test suite considered in the studies (i.e., unit, integration, or system test suite); for this reason, Table~\ref{table:benchmarks} does not specify the type of test suites considered.}



%\DONE{"Appreciate" is a little too much :)}
%From Table~\ref{table:benchmarks}, it also can be appreciated the wide variety of case studies selected in the literature, while some authors decide to assess their techniques using very simple programs such as \textit{abs}~\cite{tokumoto2016muvm}. Other authors preferred using Unix applications such as the Coreutils package~\cite{hariri2019comparing,papadakis2018mutation,chekam2017empirical}. 
%On the other hand, some authors selected very complex programs such as OpenSSL and LLVM~\cite{denisov2018mull}, but usually the experiments are exercised only on small components of these large applications.

The size of the benchmark case studies varies a lot. They include simple algorithms such as \textit{abs} (6 LOC)~\cite{tokumoto2016muvm}, large Unix utilities such as the Coreutils package~\cite{hariri2019comparing,papadakis2018mutation,chekam2017empirical}, and programs implementing complex functions such as OpenSSL and LLVM~\cite{denisov2018mull}.
However, when large and complex software systems are considered in the empirical evaluation, the evaluation concerns only a subset of the components of these large applications.
For example, in the study performed by Kintis et al.~\cite{kintis2017detecting} they considered the assessment of Vim, a Unix text editor of 362 KLOC, however, because of the size of the program, the authors decided to restrict the analysis only to a couple of components such as \texttt{spell} and \texttt{eval}, which consist of 16 and 22 KLOC, respectively. 

%\DONE{Provide an example. Describe what they did in a paper where they selected a component, otherwise the sentence above might not be understood}

%\DONE{I cannot understand what is the point of the following paragraph. Do you mean that they considered the largest case studies appearing in the table? }
\DONE{Differently from lating langauges, when you have  a list of things in english, you put a comma before the 'and'. Already fixed. ANyway, can you move the LOC in parenthesis after the name of the tool?}

Concerning the size of the selected software under test, Papadakis and Chekam~\cite{papadakis2018mutation,chekam2017empirical,papadakis2018mutant} considered the largest case studies among all the benchmarks presented in Table~\ref{table:benchmarks}. These case studies are Coreutils, Findutils, Grep, Make, and Codeflaws (83 KLOC, 18 KLOC, 9 KLOC, 35 KLOC, and 266 KLOC, respectively). 
Concerning the size of the test suite employed to evaluate the proposed approaches, again Papadakis and Chekam~\cite{papadakis2018mutation,chekam2017empirical,papadakis2018mutant} present the studies including the largest test suites, with a number of test cases ranging from 58\,131 to 122\,261.
Despite scalability remains an open problem for mutation testing, the work of Papadakis and Chekam~\cite{papadakis2018mutation,chekam2017empirical,papadakis2018mutant} shows that optimization techniques are scaling up to large software systems.

%\DONE{Put th ename of the author after "of"}
Concerning the adoption of \INDEX{industrial case studies}, the most recent work is that of Delgado et al.~\cite{delgado2018evaluation} where mutation testing has been applied to 15 functions of a Commercial Off The Shelf Component used in nuclear systems. 
Another paper evaluating the applicability of mutation testing to safety critical systems is that of Daran and Thavenod-Fosse~\cite{daran1996software}, who conducted a study to identify if mutations are correlated with real faults; the experimentation was carried out on a critical software from the civil nuclear field. 
Andrews et al.~\cite{andrews2005mutation}, instead, explored the relation between hand-seeded and real faults in the software \EMPH{Space}. Space is a software developed at the European Space Agency that it has been used as case study in software engineering papers since 1998 \cite{frankl1998further}.
Finally,  Baker and Habli~\cite{baker2012empirical} conducted experiments on two safety-critical airborne systems, C and Ada, that had satisfied the coverage requirements for certification. In their experiments, they found an effective subset of mutation operators able to detect multiple deficiencies in test suites already assessed by experts. 

%\TODO{DOnt use "Even though" at the beginning of sentences}

%\REVTWO{C41}{Even though, many efforts has been done 
\REVTWO{C41}{Despite a large deal of effort has been made to identify solutions that make mutation testing scale, empirical studies on the application of mutation testing to large industrial case studies, including space software, are missing.}
\REVTWO{C42}{Also, most of the work has been performed in academic environments, literature lacks studies on the integration of mutation testing in an industrial context.}

% carried out an empirical evaluation based on two safety-critical airborne systems that had satisfied the coverage requirements for certification. Those systems were developed using high-integrity subsets for C (MISRA C [33]) and Ada. In their experiments, they found an effective subset of mutation operators that was able to detect different deficiencies in tests suites which had already met statement and MC/DC coverage and had been manually peer-reviewed.

\input{tables/industry}

\clearpage

\section{Data-Driven Mutation Testing Benchmarks}
\label{section:industry:data}

\REVTWO{C40}{In this section,} we provide a survey of the benchmarks considered both in empirical evaluations described in research work, and in industrial cases for data-driven mutation testing.

%\TODO{What about "considered in the literature and industry" => reported in the literature}

Table~\ref{table:benchmarks_datadriven} provides the list of the case study systems reported in the literature. Similar to the previous section, for each case study we report (1) the case study (i.e., the software under test - SUT), (2) the size of the SUT (the size can be expressed in terms of bytecode/machine instructions, lines of code (LOC), or executable size (KB or MB)), (3) the size of the test suite for the SUT (unfortunately, in most cases test suite details were not available - N/A), and (4) the references to the papers that report the case study.

Table~\ref{table:benchmarks_datadriven} presents case studies for approaches relying on UML models~\cite{di2017augmenting}, block models~\cite{pham2016model}, grammars, and no models~\cite{AFL:industrialcases}.
%\DONE{FIX the following}
For our analysis, we considered empirical evaluations presented in recent papers referenced in this book. More precisely, we considered papers published between 1998 and 2017.

Concerning the size and typology of SUT, the case studies reported in Table~\ref{table:benchmarks_datadriven} vary a lot.
For instance, experimentation on block models has been applied to different types of desktop applications such as VLC, Adobe Reader, Real Player, and Windows Media Player~\cite{pham2016model} which sizes goes from 60 KB to 2.32 MB. 

%\DONE{Unfortunately, not enough. We need to report at least the recent papers that we cite for grammar. Also, AFL does not work with grammars it's an approach for no models}

Empirical studies on grammars has been performed since long ago, for instance Ghosh et al.~\cite{ghosh1998testing} carried on experimentation on a set of Windows NT utilities such as \texttt{attrib, chkdsk, comp, expand, fc, find, help, label} and \texttt{replace}.
Then, Godefroid et al.~\cite{Godefroid:GrammarBasedFuzzying:2008} experimented on grammar-based whitebox fuzzing techniques on the JavaScript Interpreter from Internet Explorer 7, a widely used component of 113\,562 machine instructions.
More recently, Appelt et al.~\cite{Appelt:SQLI:ISSTA:2014} performed experiments on HotelRS, a software to study service-oriented architectures, and on SugarCRM a customer relationship management system, a large software of about 352 KLOC.

Concerning approaches that do not require models, we report the most relevant case studies for the AFL tool~\cite{AFL:industrialcases}, the mostly known tool for this category. Particularly, AFL has been applied to a very wide range of software types, for example it has been applied on PHP programs, Web browsers (e.g., Firefox), libraries (e.g., OpenSSL), and application servers (e.g., MySQL Server). 

%\DONE{please put all the references of Di Nardo's papers}
Regarding case studies applied to space software, the only case is that of SES-DAQ, a data acquisition system that processes bytestreams of transmitted satellite data. The version of SES-DAQ used for the experimentation performed by Di Nardo et al.~\cite{di2017augmenting,di2015evolutionary,di2015generating} had a size of 32\,469 bytecode instructions.

%\DONE{Please check the following}
Contrarily to code-driven mutation, data-driven approaches have been evaluated against several commercial software systems. This is due to the fact that most data-driven approaches do not require the source code of the software under test and thus can be applied by researchers to commercial software without the need of involving the companies who developed such systems. Unfortunately, the evaluation of  data-driven approaches with space software is limited to a single case study system.

%larg an industrial context (i.e., to test a software  industry case studies is not common. The main difference with code-driven mutation is that data-driven approaches had been tried it is been much common, mainly due to the cost of data-driven mutation with respect to code-drive mutation testing approaches.}
%Contrarily to code-driven mutation, the application of data-driven approaches to real industrial software it is been much common, mainly due to the cost of data-driven mutation with respect to code-drive mutation testing approaches.

\input{tables/industry_datadriven}

\clearpage




