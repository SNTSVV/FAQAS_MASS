% !TEX root = MutationTestingSurvey.tex

\section{Automated Augmentation of Test Suites}
\label{sec:testGeneration}

%Fabrizio: teh followng is not neecessary. Two problems: (1) the MP evaluates a test suite, does not require it; (2) "may or may not" leads to unclear understanding
%The mutation process relies on the existence of a test suite containing test cases that may or may not kill the mutants.

This section describe the approaches that can be adopted to automatically generate test cases that kill mutants.
%Fabrizio:
To kill a mutant we need test cases that reach the mutation point (i.e., execute the mutated code), cause 
corruptions
%changes, or corruptions, 
in the program state right after the mutated code in comparison to the non-mutated version of the program, 
and manifest these corruptions into the program output 
%Fabrizio: an assertion is not an output
%(e.g., test assertion) 
(e.g., by producing an erroneous value in a state variable verified by a test assertion) 
thus leading to a software failure \cite{papadakis2019mutation}, these conditions are also known as the killing conditions of a mutant.
%Fabrizio: singular becuase "approaches  exist"
In the literature, there exist two groups of approaches for 
%driving automated test generation processes aimed to kill mutants, based on the above mentioned conditions. 
generating test cases that kill mutants:
%Fabrizio: "symbolic procedures" is really the right terminology?
%anywa, below symbolic execution is just part of the solutions adopted so we need a broader term
%The first group consists of techniques based on symbolic execution; the second group relies on search-based test generation techniques.
approaches based on constraint-programming, and approaches based on evolutionary computation.

%\subsection{Symbolic Test Generation Procedures}
\subsection{Test Generation based on Constraint Programming}

Techniques based on 
%symbolic procedures 
constraint programming use some form of automated reasoning (e.g., Propositional Satisfiability or Constraint Solving~\cite{SATandCPsurvey:2006})
%F: the sentence is too convoluted
% to determine the existence of input data that satisfy all the requirements of a test case to kill a mutant \cite{offutt1997automatically}. 
to derive data that satisfy all the conditions necessary to kill a mutant~\cite{offutt1997automatically}.
%F: the following sentence does not add anything to the previous one, the solution is to go into the details
%A way to prove such existence is to use constraint-based methods, in which each test killing requirement represents a constraint in a constraint system, the solution to such system will be a test case that kills the mutant \cite{demillo1991constraint}.
One possibility consists of generating constraints that capture the three killing conditions of a mutant: reachability (i.e, the test case should execute the mutated statement), necessity (i.e., the test case should cause an incorrect intermediate state if it reaches the mutated statement), and sufficiency (i.e., the final state of the mutated program should differ from that of the original program)~\cite{offutt1997automatically}. Existing approaches, differ for the strategy adopted to automatically generate these constraints from the program under test.
%F: We should say something like "Offur et al, for example automatically generate such constraints by parsing the source code of teh program." But honestly I just scanned the paper, not sure if it is the case, please check.


%Recently, 
Holling et al. \cite{holling2016nequivack} proposed to use a symbolic execution approach to identify new test cases, the idea is to execute symbolically both the original and the mutated function, and then to check if their return values are equivalent or not. 
%F: please provide a high-level description of how symbolic execution works
%F: I'm expecting a sentence as "Values are considered equivalent when ... "
Each non-equivalent value proves that a new input that makes the original and the mutated function generate different results, has been found. 
%F: PLease fix,  do not remember
To automate the generation of inputs, Holling et al.rely on ??KLEE??

We introduce an example of Holling's approach in Listing \ref{function}. The top part of Listing \ref{function} shows the function \texttt{isPositive}, which checks if an integer number is positive or not. The bottom part of Listing \ref{function} presents the mutated version of \texttt{isPositive}, 
%F: it's already mutated!!
%that mutates the relational $\geq$ into $>$.
where the relational operator $\geq$ has been replaced by the operator $>$.
To automate the generation of inputs using ?KLEE?, all the parameters need to be treated as symbolic values.
%F: "the \texttt{isPositive} parameter \texttt{num}" it's impossible to understand what is the parameter and who owns the parameter
%In Holling's approach, first, the \texttt{isPositive} parameter \texttt{num} needs to be treated as symbolic value, which is done by the function \texttt{make\_symbolic} (see Listing \ref{example}) which converts concrete variables to symbolic by considering its memory address and size. 
This is achieved by function \texttt{make\_symbolic} (see Listing \ref{example}) which converts concrete variables to symbolic by considering its memory address and size. 
%F: please fix
In  Listing \ref{example}, the parameter \texttt{num} is made symbolic in Line ??
Then, the original and mutated functions are called using the symbolic arguments (see Lines 5 and 6 in Listing \ref{function}). 
%F: with my changes I'm trying to provide a description of the process automated by the approach, and use the listing to provide a concrete example for each step of the process
%F: never write that an assertion verifies if two value are equal or not. Either "verifies that are equal" OR "verifies that are different", otherwise for teh reader is impossible to undertsand what the asser verifies
%Finally, the \texttt{assert} function of line 8 verifies if the integer return values of both functions are equal or not.
Finally, we need to introduce an assertion that verifies that the output of the two functions are different. 
In Listing \ref{example}, this is achieved by verifying that return values are different (see Line...). 
This is done because symbolic execution approaches aim to identify inputs that falsify the assertions in the program. 
An input that falsifies such an equality is thus capable of identifying inputs that lead to different output  falsifying the assertion

%In the concrete, if we consider the implementation of \texttt{isPositive} and \texttt{MUT\_isPositive}, 
In the example of Listing \ref{example},
%F: what is a "case" ?
%the symbolic execution engine will detect the case when \texttt{num} is equal to 0, input that will make both the original and mutated version to produce different outputs. 
KLEE will indicate that the return values differ when \texttt{num} is equal to zero.
A new test case exercising function \texttt{isPositive} with \texttt{num=0} should thus be added to the test suite in order to kill the mutant.
%F: "the input 0 will be added as a new test case" does not make sense, an input is not a test case
%In this case, the input 0 will be added as a new test case of the \texttt{isPositive} function.

%Fabrizio: please, put  tables and listing in separate tex files and use \input{...}
\begin{lstlisting}[style=CStyle, caption=isPositive and MUT\_isPositive functions, label=function]
int isPositive(int num){
	if (num >= 0){
		return 1;
	} else {
		return 0;
	}
}

int MUT_isPositive(int num){
	if (num > 0){
		return 1;
	} else {
		return 0;
	}
}

\end{lstlisting}

\begin{lstlisting}[style=CStyle, caption=Holling's approach for test case generation., label=example]
void test () { 
	int numSymbolic; 
	make_symbolic(&numSymbolic, sizeof(numSymbolic), "numSymbolic"); 
 	
 	int original_ret = isPositive(numSymbolic); 
	int transformed_ret = MUT_isPositive(numSymbolic); 
 
	assert(original_ret == transformed_ret); 
} 
 
int main(int argc, char* argv[]) { 
	test (); 
	return 0; 
}
\end{lstlisting}

%F: can't we say anything more? For example, which bounded model checking approach they use?
Similarly to Holling's approach, Riener et al. \cite{riener2011test} proposed to use bounded model checking techniques to search for these counter examples.
%F: you should explain what is bounded-model checking. If Riener do not provide background you can check my ISSTA'14 paper (Verification-Aided Regression Testing), there is a background section (2) on bounded model checking that starts with "The idea in BMC is to represent ..."
Compared to symbolic execution, one advantage of bounded model checking is that it does not require to process all the functions

%F: in the following it is not clear the objective of the work. Shall we say 
%To reduce the time required by the symbolic execution process, which needs to be performed against all the mutants of the software, Papadakis et al. \cite{papadakis2011automatically, papadakis2010towards} propose to combine symbolic execution techniques and mutant schemata to automatically generate test cases targeting the killing conditions induced by the different mutants embedded into the same executable. The approach targets weak mutation testing and may not generalize to firm mutation testing. Indeed, ensuring the sufficiency property (i.e., verify that canges are propagated to outputs) for multiple mutants might lead to scalability issues not addressed by the proposed approach.
Papadakis et al. \cite{papadakis2011automatically, papadakis2010towards} proposed an approach targeting weak mutation testing, in their study they suggest to use symbolic execution techniques along with mutant schemata to automatically generate test cases targeting the killing conditions induced by the different mutants embedded into the same executable.

\subsection{Test Generation based on Evolutionary Computation}

Test generation approaches based on evolutionary computation typically rely on  population-based metaheuristic optimization algorithms~\cite{}. 
%F: "This class of solutions formulate" formulate what? this verb requires an object, the sentence is broken
%F: anyway I cannot understand the sentence, what does it mean that "an input leads to a strong mutation" ?
%This class of solutions formulate and search for program inputs, under the guidance of a fitness function, that could lead to strong mutations \cite{harman2011strong}. 
%F: "that" can be used only to specify a subcase of an object. In this case the subset of inputs that lead to 
They search for program inputs that could kill mutants under the guidance of a fitness function~\cite{harman2011strong}. 
%"concern" ?
The main concern of these methods is the definition of fitness functions that capture the killing conditions of a mutant, and that identifies test inputs that satisfies those conditions.
They fitness function captures the killing conditions of a mutant

Ayari et al. \cite{ayari2007automatic} proposed to use an evolutionary approach based on ant colon optimization for automatic test input data generation on mutation testing. 
%F: please describe what is ant colony optimization, in teh paper there should be a background sectiomn that you should be able to reuse
The approach takes an existing test case and produces a new test case by slightly modifying its inputs. 
%F: you have to provide more details what does it mean "close"?
The fitness function measures how close is this new test case from covering the mutated statement.

%F: it seems that both proposed the same approach, which is strange
%F: how can you compute a distance between a test case (which is a sequence of inputs) and a program state ? maybe the distance between the program state achieved in two test case executions?
%Papadakis et al. \cite{papadakis2011automatically} and then Fraser and Arcuri \cite{fraser2015achieving} proposed a fitness function defined by the distance between the test case and a new corrupted program state induced by the mutant. 
Papadakis et al. \cite{papadakis2011automatically} and Fraser and Arcuri \cite{fraser2015achieving} rely on fitness functions that capture the distance between FIMXE
%F: you should first describe papadakis approach
%F: "according to" I cannot understand what it means do you mean the following
%Fraser and Arcuri \cite{fraser2015achieving} propose to use distinct distance metrics tailored to teh specific operator used to generate the mutants.
The approach presented by Fraser and Arcuri \cite{fraser2015achieving} measures the distance according to the mutation operator being applied, 
%F: we need more examples for multiple operators. Also, you should clarify why distinct metrics are needed
for example if the deletion operator changes the program state (i.e., values on the stack are different at the mutation point) the distance is 0, otherwise the given value is 1.

%F: the followig is not useful. You should clarify hwat is teh contribution, how does it differ from the previous approaches?
In a similar way, Patrick et al. \cite{patrick2013using} proposed an evolutionary algorithm to improve the inputs produced by random testing according to their ability in killing mutants. 

\endinput

\subsubsection{Symbolic execution test generation} % (fold)

\begin{itemize}

	\item Dynamic symbolic execution approaches: embed the mutant killing conditions within the executable program and guide test generation towards these conditions.

	\item Papadakis et al. \cite{papadakis2011automatically, papadakis2010towards}:
	Embed mutant infection conditions (cause a corruption to the program state) within the schematic functions that are produced by mutant schemata. Basically, we have all mutants encoded in a single executable with their killing conditions. Then, using dynamic symbolic execution we can directly produce test cases that targets those infection conditions.
\end{itemize}


